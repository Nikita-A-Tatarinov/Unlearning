{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/hice1/dk305/.conda/envs/tofu/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "import torch\n",
    "import evaluate\n",
    "from evaluate import load\n",
    "from tqdm import tqdm\n",
    "import numpy as np\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from evaluate import logging\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████| 3/3 [04:22<00:00, 87.43s/it]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(32001, 4096, padding_idx=32000)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaSdpaAttention(\n",
       "          (q_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (v_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (o_proj): Linear(in_features=4096, out_features=4096, bias=False)\n",
       "          (rotary_emb): LlamaRotaryEmbedding()\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (up_proj): Linear(in_features=4096, out_features=11008, bias=False)\n",
       "          (down_proj): Linear(in_features=11008, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-06)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-06)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=32001, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "\n",
    "###change to your model and tokenizer\n",
    "# bnb_config = BitsAndBytesConfig(\n",
    "#         load_in_4bit=True, bnb_4bit_quant_type=\"nf4\", bnb_4bit_compute_dtype=\"float16\", bnb_4bit_use_double_quant=True\n",
    "#     )\n",
    "\n",
    "# model_path = \"/storage/ice1/6/8/dk305/Unlearning/SimPO/models/alpaca-7b-reproduced/unlearned/V2_1GPU_simnpo_grad_diff_1e-05_forget05_epoch5_batch1_accum4_beta2.5_gamma0.0_grad_diff_coeff1.0_reffine_tuned_evalsteps_per_epoch_seed1001_1/checkpoint-2750\"\n",
    "# model_path = \"/home/hice1/dk305/scratch/Unlearning/SimPO/models/alpaca-7b-reproduced/unlearned/V3_1GPU_simnpo_grad_diff_1e-05_forget05_epoch10_batch1_accum4_beta2.5_gamma0.0_grad_diff_coeff1.0_reffine_tuned_evalsteps_per_epoch_seed1001_1\"\n",
    "model_path = \"/home/hice1/dk305/scratch/Unlearning/SimPO/models/alpaca-7b-reproduced/unlearned/V3_1GPU_simnpo_grad_diff_1e-05_forget05_epoch10_batch1_accum4_beta2.5_gamma0.0_grad_diff_coeff1.0_reffine_tuned_evalsteps_per_epoch_seed1001_1\"\n",
    "\n",
    "model = AutoModelForCausalLM.from_pretrained(model_path, device_map='auto')\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_path)\n",
    "model.to(device)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating train split: 100%|██████████| 2662/2662 [00:03<00:00, 744.73 examples/s]\n",
      "Generating test split: 100%|██████████| 5153/5153 [00:00<00:00, 468437.73 examples/s]\n",
      "Generating validation split: 100%|██████████| 4869/4869 [00:00<00:00, 510003.40 examples/s]\n"
     ]
    }
   ],
   "source": [
    "lambada_dataset = load_dataset(\"lambada\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Dataset: 100%|█████████▉| 900/904 [14:41<00:03,  1.02it/s]\n"
     ]
    }
   ],
   "source": [
    "# Tokenize the entire text from the test dataset\n",
    "encodings = tokenizer(\"\\n\\n\".join(lambada_dataset['test']['text']), return_tensors=\"pt\")\n",
    "\n",
    "max_length = model.config.max_position_embeddings\n",
    "stride = 512  # Sliding window stride\n",
    "seq_len = encodings.input_ids.size(1)  # Total sequence length\n",
    "\n",
    "nlls = []  # List to store negative log likelihoods\n",
    "prev_end_loc = 0  # Initialize previous end location\n",
    "\n",
    "# Iterate over the dataset in chunks using a sliding window\n",
    "for begin_loc in tqdm(range(0, seq_len, stride), desc=\"Processing Dataset\"):\n",
    "\n",
    "    end_loc = min(begin_loc + max_length, seq_len)\n",
    "    trg_len = end_loc - prev_end_loc  # Calculate target length for the chunk\n",
    "\n",
    "    # Slice the input tensor for the current chunk\n",
    "    input_ids = encodings.input_ids[:, begin_loc:end_loc].to(device)\n",
    "    target_ids = input_ids.clone()\n",
    "    \n",
    "    # Set the target labels to -100 for the tokens we don't want to calculate loss over\n",
    "    target_ids[:, :-trg_len] = -100\n",
    "\n",
    "    # Compute the loss using the model (no gradient calculation)\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=target_ids)\n",
    "        neg_log_likelihood = outputs.loss  # This is the loss for the current chunk\n",
    "\n",
    "    # Append the loss to the list of NLLs\n",
    "    nlls.append(neg_log_likelihood.item())\n",
    "\n",
    "    prev_end_loc = end_loc  # Update previous end location\n",
    "\n",
    "    if end_loc == seq_len:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity for LAMBADA dataset: 15.2909\n"
     ]
    }
   ],
   "source": [
    "# Calculate perplexity\n",
    "mean_nll = torch.tensor(nlls).mean()  # Mean negative log likelihood\n",
    "ppl = torch.exp(mean_nll)  # Perplexity is the exponent of the mean NLL\n",
    "\n",
    "# Print the calculated perplexity\n",
    "print(f\"Perplexity for LAMBADA dataset: {ppl:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating test split: 100%|██████████| 4358/4358 [00:00<00:00, 542075.23 examples/s]\n",
      "Generating train split: 100%|██████████| 36718/36718 [00:00<00:00, 841101.11 examples/s]\n",
      "Generating validation split: 100%|██████████| 3760/3760 [00:00<00:00, 537035.45 examples/s]\n"
     ]
    }
   ],
   "source": [
    "# Load WikiText-2 dataset\n",
    "validation_dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"validation\")\n",
    "test_dataset = load_dataset(\"wikitext\", \"wikitext-2-raw-v1\", split=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 99%|█████████▉| 663/667 [10:47<00:03,  1.02it/s]\n"
     ]
    }
   ],
   "source": [
    "encodings = tokenizer(\"\\n\\n\".join(test_dataset[\"text\"]), return_tensors=\"pt\")\n",
    "max_length = model.config.max_position_embeddings\n",
    "stride = 512\n",
    "seq_len = encodings.input_ids.size(1)\n",
    "\n",
    "nlls = []\n",
    "prev_end_loc = 0\n",
    "for begin_loc in tqdm(range(0, seq_len, stride)):\n",
    "    end_loc = min(begin_loc + max_length, seq_len)\n",
    "    trg_len = end_loc - prev_end_loc  # may be different from stride on last loop\n",
    "    input_ids = encodings.input_ids[:, begin_loc:end_loc].to(device)\n",
    "    target_ids = input_ids.clone()\n",
    "    target_ids[:, :-trg_len] = -100\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model(input_ids, labels=target_ids)\n",
    "\n",
    "        # loss is calculated using CrossEntropyLoss which averages over valid labels\n",
    "        # N.B. the model only calculates loss over trg_len - 1 labels, because it internally shifts the labels\n",
    "        # to the left by 1.\n",
    "        neg_log_likelihood = outputs.loss\n",
    "\n",
    "    nlls.append(neg_log_likelihood)\n",
    "\n",
    "    prev_end_loc = end_loc\n",
    "    if end_loc == seq_len:\n",
    "        break\n",
    "\n",
    "ppl = torch.exp(torch.stack(nlls).mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Perplexity on WikiText Test: 5.6689\n"
     ]
    }
   ],
   "source": [
    "print(f\"Perplexity on WikiText Test: {ppl:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "DLT2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
