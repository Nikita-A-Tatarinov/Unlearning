{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num. devices: \n",
      "2\n",
      "NVIDIA L40S\n"
     ]
    }
   ],
   "source": [
    "import torch \n",
    "\n",
    "if torch.cuda.is_available(): \n",
    "    print(\"Num. devices: \") \n",
    "    print(torch.cuda.device_count()) \n",
    "    print(torch.cuda.get_device_name(0)) \n",
    "else: \n",
    "    print(\"No GPU available\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading checkpoint shards: 100%|██████████████████| 7/7 [01:33<00:00, 13.42s/it]\n",
      "Loading checkpoint shards: 100%|██████████████████| 7/7 [00:02<00:00,  2.59it/s]\n",
      "====rmu Config====\n",
      "model_name_or_path=PKU-Alignment/alpaca-7b-reproduced\n",
      "module_str={model_name}.model.layers[{layer_id}]\n",
      "output_dir=models/alpaca_rmu\n",
      "retain_corpora=['SafeRLHF-corpora/safe_train']\n",
      "forget_corpora=['SafeRLHF-corpora/Privacy_Violation_train']\n",
      "alpha=[1200.0]\n",
      "steering_coeffs=6.5\n",
      "lr=1e-05\n",
      "min_len=0\n",
      "max_len=2000\n",
      "batch_size=8\n",
      "max_num_batches=300\n",
      "layer_id=7\n",
      "layer_ids=[5, 6, 7]\n",
      "param_ids=[6]\n",
      "seed=42\n",
      "verbose=True\n",
      "steering_coeff_list=[6.5]\n",
      "=====\n",
      "/home/hice1/jli928/.conda/envs/model-unlearning/lib/python3.10/site-packages/transformers/optimization.py:591: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
      "  warnings.warn(\n",
      "======= Epoch 0 =======\n",
      "  0%|                                                   | 0/300 [00:00<?, ?it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 269, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1200 | unlearn_loss: 1200 | retain_loss: 0 | param_change: 1.669e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1264.0\n",
      "Topic 0 frozen_forget_activations.norm= 1264.0\n",
      "Topic 0 updated_retain_activations.norm= 1880.0\n",
      "Topic 0 frozen_retain_activations.norm= 1880.0\n",
      "  0%|▏                                          | 1/300 [00:01<09:44,  1.95s/it]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 311, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1136 | unlearn_loss: 1136 | retain_loss: 0.00209 | param_change: 1.907e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1168.0\n",
      "Topic 0 frozen_forget_activations.norm= 1168.0\n",
      "Topic 0 updated_retain_activations.norm= 1048.0\n",
      "Topic 0 frozen_retain_activations.norm= 1048.0\n",
      "  1%|▎                                          | 2/300 [00:02<06:36,  1.33s/it]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 238, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1336 | unlearn_loss: 1336 | retain_loss: 0.001999 | param_change: 1.907e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1336.0\n",
      "Topic 0 frozen_forget_activations.norm= 1336.0\n",
      "Topic 0 updated_retain_activations.norm= 1504.0\n",
      "Topic 0 frozen_retain_activations.norm= 1504.0\n",
      "  1%|▍                                          | 3/300 [00:03<05:17,  1.07s/it]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 301, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1240 | unlearn_loss: 1240 | retain_loss: 0.002411 | param_change: 1.717e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1304.0\n",
      "Topic 0 frozen_forget_activations.norm= 1304.0\n",
      "Topic 0 updated_retain_activations.norm= 1576.0\n",
      "Topic 0 frozen_retain_activations.norm= 1576.0\n",
      "  1%|▌                                          | 4/300 [00:04<04:52,  1.01it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 250, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1288 | unlearn_loss: 1288 | retain_loss: 0.003052 | param_change: 1.597e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1328.0\n",
      "Topic 0 frozen_forget_activations.norm= 1328.0\n",
      "Topic 0 updated_retain_activations.norm= 1280.0\n",
      "Topic 0 frozen_retain_activations.norm= 1280.0\n",
      "  2%|▋                                          | 5/300 [00:05<04:30,  1.09it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 273, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 936 | unlearn_loss: 936 | retain_loss: 0.003159 | param_change: 1.895e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 884.0\n",
      "Topic 0 frozen_forget_activations.norm= 884.0\n",
      "Topic 0 updated_retain_activations.norm= 1248.0\n",
      "Topic 0 frozen_retain_activations.norm= 1248.0\n",
      "  2%|▊                                          | 6/300 [00:06<04:25,  1.11it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 271, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1032 | unlearn_loss: 1032 | retain_loss: 0.003662 | param_change: 1.955e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 980.0\n",
      "Topic 0 frozen_forget_activations.norm= 980.0\n",
      "Topic 0 updated_retain_activations.norm= 1408.0\n",
      "Topic 0 frozen_retain_activations.norm= 1408.0\n",
      "  2%|█                                          | 7/300 [00:06<04:16,  1.14it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 230, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1120 | unlearn_loss: 1120 | retain_loss: 0.003601 | param_change: 1.46e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1152.0\n",
      "Topic 0 frozen_forget_activations.norm= 1152.0\n",
      "Topic 0 updated_retain_activations.norm= 1768.0\n",
      "Topic 0 frozen_retain_activations.norm= 1768.0\n",
      "  3%|█▏                                         | 8/300 [00:07<04:16,  1.14it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 261, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1064 | unlearn_loss: 1064 | retain_loss: 0.005035 | param_change: 1.931e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1024.0\n",
      "Topic 0 frozen_forget_activations.norm= 1024.0\n",
      "Topic 0 updated_retain_activations.norm= 888.0\n",
      "Topic 0 frozen_retain_activations.norm= 888.0\n",
      "  3%|█▎                                         | 9/300 [00:08<04:09,  1.17it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 243, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1400 | unlearn_loss: 1400 | retain_loss: 0.005798 | param_change: 1.538e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1448.0\n",
      "Topic 0 frozen_forget_activations.norm= 1448.0\n",
      "Topic 0 updated_retain_activations.norm= 1024.0\n",
      "Topic 0 frozen_retain_activations.norm= 1024.0\n",
      "  3%|█▍                                        | 10/300 [00:09<04:10,  1.16it/s]loss: 1120 | unlearn_loss: 1120 | retain_loss: 0.005829 | param_change: 1.264e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1192.0\n",
      "Topic 0 frozen_forget_activations.norm= 1192.0\n",
      "Topic 0 updated_retain_activations.norm= 1176.0\n",
      "Topic 0 frozen_retain_activations.norm= 1176.0\n",
      "  4%|█▌                                        | 11/300 [00:10<04:05,  1.18it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 357, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1488 | unlearn_loss: 1488 | retain_loss: 0.006012 | param_change: 1.347e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1680.0\n",
      "Topic 0 frozen_forget_activations.norm= 1680.0\n",
      "Topic 0 updated_retain_activations.norm= 1168.0\n",
      "Topic 0 frozen_retain_activations.norm= 1168.0\n",
      "  4%|█▋                                        | 12/300 [00:11<04:20,  1.11it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 258, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1128 | unlearn_loss: 1128 | retain_loss: 0.0047 | param_change: 1.502e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1144.0\n",
      "Topic 0 frozen_forget_activations.norm= 1144.0\n",
      "Topic 0 updated_retain_activations.norm= 1672.0\n",
      "Topic 0 frozen_retain_activations.norm= 1672.0\n",
      "  4%|█▊                                        | 13/300 [00:12<04:15,  1.12it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 282, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1208 | unlearn_loss: 1208 | retain_loss: 0.006287 | param_change: 1.311e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1296.0\n",
      "Topic 0 frozen_forget_activations.norm= 1296.0\n",
      "Topic 0 updated_retain_activations.norm= 1128.0\n",
      "Topic 0 frozen_retain_activations.norm= 1128.0\n",
      "  5%|█▉                                        | 14/300 [00:13<04:21,  1.09it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 236, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1144 | unlearn_loss: 1144 | retain_loss: 0.006287 | param_change: 1.341e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1184.0\n",
      "Topic 0 frozen_forget_activations.norm= 1184.0\n",
      "Topic 0 updated_retain_activations.norm= 1712.0\n",
      "Topic 0 frozen_retain_activations.norm= 1712.0\n",
      "  5%|██                                        | 15/300 [00:14<04:29,  1.06it/s]loss: 1456 | unlearn_loss: 1456 | retain_loss: 0.006714 | param_change: 1.353e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1576.0\n",
      "Topic 0 frozen_forget_activations.norm= 1576.0\n",
      "Topic 0 updated_retain_activations.norm= 912.0\n",
      "Topic 0 frozen_retain_activations.norm= 912.0\n",
      "  5%|██▏                                       | 16/300 [00:14<04:12,  1.12it/s]loss: 1280 | unlearn_loss: 1280 | retain_loss: 0.007446 | param_change: 1.425e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1320.0\n",
      "Topic 0 frozen_forget_activations.norm= 1320.0\n",
      "Topic 0 updated_retain_activations.norm= 1632.0\n",
      "Topic 0 frozen_retain_activations.norm= 1632.0\n",
      "  6%|██▍                                       | 17/300 [00:15<04:22,  1.08it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 299, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1256 | unlearn_loss: 1256 | retain_loss: 0.007355 | param_change: 1.55e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1272.0\n",
      "Topic 0 frozen_forget_activations.norm= 1272.0\n",
      "Topic 0 updated_retain_activations.norm= 1152.0\n",
      "Topic 0 frozen_retain_activations.norm= 1152.0\n",
      "  6%|██▌                                       | 18/300 [00:16<04:22,  1.07it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 342, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1352 | unlearn_loss: 1352 | retain_loss: 0.007233 | param_change: 1.466e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1456.0\n",
      "Topic 0 frozen_forget_activations.norm= 1456.0\n",
      "Topic 0 updated_retain_activations.norm= 1128.0\n",
      "Topic 0 frozen_retain_activations.norm= 1128.0\n",
      "  6%|██▋                                       | 19/300 [00:17<04:23,  1.07it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 225, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 960 | unlearn_loss: 960 | retain_loss: 0.004578 | param_change: 1.597e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 944.0\n",
      "Topic 0 frozen_forget_activations.norm= 944.0\n",
      "Topic 0 updated_retain_activations.norm= 1992.0\n",
      "Topic 0 frozen_retain_activations.norm= 1992.0\n",
      "  7%|██▊                                       | 20/300 [00:19<04:53,  1.05s/it]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 244, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1072 | unlearn_loss: 1072 | retain_loss: 0.00824 | param_change: 1.872e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1048.0\n",
      "Topic 0 frozen_forget_activations.norm= 1048.0\n",
      "Topic 0 updated_retain_activations.norm= 960.0\n",
      "Topic 0 frozen_retain_activations.norm= 960.0\n",
      "  7%|██▉                                       | 21/300 [00:19<04:26,  1.05it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 235, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1072 | unlearn_loss: 1072 | retain_loss: 0.0108 | param_change: 1.359e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1064.0\n",
      "Topic 0 frozen_forget_activations.norm= 1064.0\n",
      "Topic 0 updated_retain_activations.norm= 1544.0\n",
      "Topic 0 frozen_retain_activations.norm= 1544.0\n",
      "  7%|███                                       | 22/300 [00:20<04:07,  1.12it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 379, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1512 | unlearn_loss: 1512 | retain_loss: 0.005829 | param_change: 1.276e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1768.0\n",
      "Topic 0 frozen_forget_activations.norm= 1768.0\n",
      "Topic 0 updated_retain_activations.norm= 1944.0\n",
      "Topic 0 frozen_retain_activations.norm= 1944.0\n",
      "  8%|███▏                                      | 23/300 [00:22<04:54,  1.06s/it]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 302, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1240 | unlearn_loss: 1240 | retain_loss: 0.008789 | param_change: 1.276e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1320.0\n",
      "Topic 0 frozen_forget_activations.norm= 1320.0\n",
      "Topic 0 updated_retain_activations.norm= 1480.0\n",
      "Topic 0 frozen_retain_activations.norm= 1480.0\n",
      "  8%|███▎                                      | 24/300 [00:23<04:48,  1.04s/it]loss: 1344 | unlearn_loss: 1344 | retain_loss: 0.008728 | param_change: 1.341e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1456.0\n",
      "Topic 0 frozen_forget_activations.norm= 1456.0\n",
      "Topic 0 updated_retain_activations.norm= 1408.0\n",
      "Topic 0 frozen_retain_activations.norm= 1408.0\n",
      "  8%|███▌                                      | 25/300 [00:24<04:35,  1.00s/it]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 217, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1192 | unlearn_loss: 1192 | retain_loss: 0.0108 | param_change: 1.246e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1232.0\n",
      "Topic 0 frozen_forget_activations.norm= 1232.0\n",
      "Topic 0 updated_retain_activations.norm= 1088.0\n",
      "Topic 0 frozen_retain_activations.norm= 1088.0\n",
      "  9%|███▋                                      | 26/300 [00:24<04:12,  1.08it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 341, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1328 | unlearn_loss: 1328 | retain_loss: 0.007874 | param_change: 1.538e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1384.0\n",
      "Topic 0 frozen_forget_activations.norm= 1384.0\n",
      "Topic 0 updated_retain_activations.norm= 1552.0\n",
      "Topic 0 frozen_retain_activations.norm= 1552.0\n",
      "  9%|███▊                                      | 27/300 [00:25<04:31,  1.01it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 284, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1168 | unlearn_loss: 1168 | retain_loss: 0.008911 | param_change: 1.508e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1184.0\n",
      "Topic 0 frozen_forget_activations.norm= 1184.0\n",
      "Topic 0 updated_retain_activations.norm= 1568.0\n",
      "Topic 0 frozen_retain_activations.norm= 1568.0\n",
      "  9%|███▉                                      | 28/300 [00:27<04:52,  1.08s/it]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 208, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1048 | unlearn_loss: 1048 | retain_loss: 0.01129 | param_change: 1.347e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1056.0\n",
      "Topic 0 frozen_forget_activations.norm= 1056.0\n",
      "Topic 0 updated_retain_activations.norm= 1384.0\n",
      "Topic 0 frozen_retain_activations.norm= 1384.0\n",
      " 10%|████                                      | 29/300 [00:27<04:26,  1.02it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 211, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1112 | unlearn_loss: 1112 | retain_loss: 0.01007 | param_change: 1.287e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1120.0\n",
      "Topic 0 frozen_forget_activations.norm= 1120.0\n",
      "Topic 0 updated_retain_activations.norm= 1120.0\n",
      "Topic 0 frozen_retain_activations.norm= 1120.0\n",
      " 10%|████▏                                     | 30/300 [00:28<04:06,  1.10it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 224, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1136 | unlearn_loss: 1136 | retain_loss: 0.009094 | param_change: 1.341e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1168.0\n",
      "Topic 0 frozen_forget_activations.norm= 1168.0\n",
      "Topic 0 updated_retain_activations.norm= 1696.0\n",
      "Topic 0 frozen_retain_activations.norm= 1696.0\n",
      " 10%|████▎                                     | 31/300 [00:29<03:52,  1.16it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 253, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1336 | unlearn_loss: 1336 | retain_loss: 0.01031 | param_change: 1.144e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1432.0\n",
      "Topic 0 frozen_forget_activations.norm= 1432.0\n",
      "Topic 0 updated_retain_activations.norm= 1424.0\n",
      "Topic 0 frozen_retain_activations.norm= 1424.0\n",
      " 11%|████▍                                     | 32/300 [00:30<03:54,  1.14it/s]loss: 1120 | unlearn_loss: 1120 | retain_loss: 0.009094 | param_change: 1.472e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1112.0\n",
      "Topic 0 frozen_forget_activations.norm= 1112.0\n",
      "Topic 0 updated_retain_activations.norm= 1680.0\n",
      "Topic 0 frozen_retain_activations.norm= 1680.0\n",
      " 11%|████▌                                     | 33/300 [00:31<04:08,  1.08it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 347, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1424 | unlearn_loss: 1424 | retain_loss: 0.01288 | param_change: 1.413e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1552.0\n",
      "Topic 0 frozen_forget_activations.norm= 1552.0\n",
      "Topic 0 updated_retain_activations.norm= 1552.0\n",
      "Topic 0 frozen_retain_activations.norm= 1552.0\n",
      " 11%|████▊                                     | 34/300 [00:32<04:20,  1.02it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 268, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1272 | unlearn_loss: 1272 | retain_loss: 0.01306 | param_change: 1.484e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1320.0\n",
      "Topic 0 frozen_forget_activations.norm= 1320.0\n",
      "Topic 0 updated_retain_activations.norm= 836.0\n",
      "Topic 0 frozen_retain_activations.norm= 836.0\n",
      " 12%|████▉                                     | 35/300 [00:33<04:06,  1.08it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 350, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1280 | unlearn_loss: 1280 | retain_loss: 0.008972 | param_change: 1.347e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1384.0\n",
      "Topic 0 frozen_forget_activations.norm= 1384.0\n",
      "Topic 0 updated_retain_activations.norm= 1864.0\n",
      "Topic 0 frozen_retain_activations.norm= 1864.0\n",
      " 12%|█████                                     | 36/300 [00:34<04:20,  1.01it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 216, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1032 | unlearn_loss: 1032 | retain_loss: 0.01282 | param_change: 1.377e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1008.0\n",
      "Topic 0 frozen_forget_activations.norm= 1008.0\n",
      "Topic 0 updated_retain_activations.norm= 1184.0\n",
      "Topic 0 frozen_retain_activations.norm= 1184.0\n",
      " 12%|█████▏                                    | 37/300 [00:35<04:01,  1.09it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 337, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1432 | unlearn_loss: 1432 | retain_loss: 0.01251 | param_change: 1.52e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1544.0\n",
      "Topic 0 frozen_forget_activations.norm= 1544.0\n",
      "Topic 0 updated_retain_activations.norm= 1448.0\n",
      "Topic 0 frozen_retain_activations.norm= 1448.0\n",
      " 13%|█████▎                                    | 38/300 [00:36<04:08,  1.06it/s]loss: 1160 | unlearn_loss: 1160 | retain_loss: 0.01331 | param_change: 1.645e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1152.0\n",
      "Topic 0 frozen_forget_activations.norm= 1152.0\n",
      "Topic 0 updated_retain_activations.norm= 1368.0\n",
      "Topic 0 frozen_retain_activations.norm= 1368.0\n",
      " 13%|█████▍                                    | 39/300 [00:37<04:01,  1.08it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 319, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1296 | unlearn_loss: 1296 | retain_loss: 0.01086 | param_change: 1.305e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1424.0\n",
      "Topic 0 frozen_forget_activations.norm= 1424.0\n",
      "Topic 0 updated_retain_activations.norm= 1624.0\n",
      "Topic 0 frozen_retain_activations.norm= 1624.0\n",
      " 13%|█████▌                                    | 40/300 [00:38<04:11,  1.03it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 361, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1496 | unlearn_loss: 1496 | retain_loss: 0.01074 | param_change: 1.431e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1672.0\n",
      "Topic 0 frozen_forget_activations.norm= 1672.0\n",
      "Topic 0 updated_retain_activations.norm= 1704.0\n",
      "Topic 0 frozen_retain_activations.norm= 1704.0\n",
      " 14%|█████▋                                    | 41/300 [00:39<04:13,  1.02it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 275, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1048 | unlearn_loss: 1048 | retain_loss: 0.0144 | param_change: 1.645e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1016.0\n",
      "Topic 0 frozen_forget_activations.norm= 1016.0\n",
      "Topic 0 updated_retain_activations.norm= 992.0\n",
      "Topic 0 frozen_retain_activations.norm= 992.0\n",
      " 14%|█████▉                                    | 42/300 [00:40<04:05,  1.05it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 305, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1096 | unlearn_loss: 1096 | retain_loss: 0.01306 | param_change: 1.538e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1104.0\n",
      "Topic 0 frozen_forget_activations.norm= 1104.0\n",
      "Topic 0 updated_retain_activations.norm= 1048.0\n",
      "Topic 0 frozen_retain_activations.norm= 1048.0\n",
      " 14%|██████                                    | 43/300 [00:41<04:05,  1.04it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 231, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1560 | unlearn_loss: 1560 | retain_loss: 0.013 | param_change: 1.371e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1648.0\n",
      "Topic 0 frozen_forget_activations.norm= 1648.0\n",
      "Topic 0 updated_retain_activations.norm= 1608.0\n",
      "Topic 0 frozen_retain_activations.norm= 1608.0\n",
      " 15%|██████▏                                   | 44/300 [00:41<03:50,  1.11it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 303, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1432 | unlearn_loss: 1432 | retain_loss: 0.01624 | param_change: 1.645e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1504.0\n",
      "Topic 0 frozen_forget_activations.norm= 1504.0\n",
      "Topic 0 updated_retain_activations.norm= 704.0\n",
      "Topic 0 frozen_retain_activations.norm= 704.0\n",
      " 15%|██████▎                                   | 45/300 [00:42<03:44,  1.14it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 327, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1400 | unlearn_loss: 1400 | retain_loss: 0.0144 | param_change: 1.574e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1528.0\n",
      "Topic 0 frozen_forget_activations.norm= 1528.0\n",
      "Topic 0 updated_retain_activations.norm= 1200.0\n",
      "Topic 0 frozen_retain_activations.norm= 1200.0\n",
      " 15%|██████▍                                   | 46/300 [00:43<03:44,  1.13it/s]loss: 1264 | unlearn_loss: 1264 | retain_loss: 0.01147 | param_change: 1.574e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1240.0\n",
      "Topic 0 frozen_forget_activations.norm= 1240.0\n",
      "Topic 0 updated_retain_activations.norm= 1696.0\n",
      "Topic 0 frozen_retain_activations.norm= 1696.0\n",
      " 16%|██████▌                                   | 47/300 [00:44<03:50,  1.10it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 483, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1456 | unlearn_loss: 1456 | retain_loss: 0.01251 | param_change: 1.037e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1864.0\n",
      "Topic 0 frozen_forget_activations.norm= 1864.0\n",
      "Topic 0 updated_retain_activations.norm= 1408.0\n",
      "Topic 0 frozen_retain_activations.norm= 1408.0\n",
      " 16%|██████▋                                   | 48/300 [00:45<04:28,  1.07s/it]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 283, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1288 | unlearn_loss: 1288 | retain_loss: 0.01257 | param_change: 1.538e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1296.0\n",
      "Topic 0 frozen_forget_activations.norm= 1296.0\n",
      "Topic 0 updated_retain_activations.norm= 1112.0\n",
      "Topic 0 frozen_retain_activations.norm= 1112.0\n",
      " 16%|██████▊                                   | 49/300 [00:46<04:13,  1.01s/it]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 295, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1400 | unlearn_loss: 1400 | retain_loss: 0.01508 | param_change: 1.228e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1520.0\n",
      "Topic 0 frozen_forget_activations.norm= 1520.0\n",
      "Topic 0 updated_retain_activations.norm= 1096.0\n",
      "Topic 0 frozen_retain_activations.norm= 1096.0\n",
      " 17%|███████                                   | 50/300 [00:47<03:56,  1.06it/s]loss: 1416 | unlearn_loss: 1416 | retain_loss: 0.01337 | param_change: 1.341e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1472.0\n",
      "Topic 0 frozen_forget_activations.norm= 1472.0\n",
      "Topic 0 updated_retain_activations.norm= 1304.0\n",
      "Topic 0 frozen_retain_activations.norm= 1304.0\n",
      " 17%|███████▏                                  | 51/300 [00:48<03:46,  1.10it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 297, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1304 | unlearn_loss: 1304 | retain_loss: 0.01349 | param_change: 1.472e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1328.0\n",
      "Topic 0 frozen_forget_activations.norm= 1328.0\n",
      "Topic 0 updated_retain_activations.norm= 1368.0\n",
      "Topic 0 frozen_retain_activations.norm= 1368.0\n",
      " 17%|███████▎                                  | 52/300 [00:49<03:44,  1.11it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 285, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1320 | unlearn_loss: 1320 | retain_loss: 0.03296 | param_change: 1.436e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1368.0\n",
      "Topic 0 frozen_forget_activations.norm= 1368.0\n",
      "Topic 0 updated_retain_activations.norm= 1012.0\n",
      "Topic 0 frozen_retain_activations.norm= 1012.0\n",
      " 18%|███████▍                                  | 53/300 [00:50<03:41,  1.11it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 265, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1200 | unlearn_loss: 1200 | retain_loss: 0.01068 | param_change: 1.574e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1216.0\n",
      "Topic 0 frozen_forget_activations.norm= 1216.0\n",
      "Topic 0 updated_retain_activations.norm= 1856.0\n",
      "Topic 0 frozen_retain_activations.norm= 1856.0\n",
      " 18%|███████▌                                  | 54/300 [00:51<04:13,  1.03s/it]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 219, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 924 | unlearn_loss: 924 | retain_loss: 0.01318 | param_change: 1.478e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 896.0\n",
      "Topic 0 frozen_forget_activations.norm= 896.0\n",
      "Topic 0 updated_retain_activations.norm= 1600.0\n",
      "Topic 0 frozen_retain_activations.norm= 1600.0\n",
      " 18%|███████▋                                  | 55/300 [00:52<04:11,  1.02s/it]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 245, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1128 | unlearn_loss: 1128 | retain_loss: 0.01422 | param_change: 1.478e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1112.0\n",
      "Topic 0 frozen_forget_activations.norm= 1112.0\n",
      "Topic 0 updated_retain_activations.norm= 1104.0\n",
      "Topic 0 frozen_retain_activations.norm= 1104.0\n",
      " 19%|███████▊                                  | 56/300 [00:53<03:49,  1.06it/s]loss: 1248 | unlearn_loss: 1248 | retain_loss: 0.01917 | param_change: 1.502e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1272.0\n",
      "Topic 0 frozen_forget_activations.norm= 1272.0\n",
      "Topic 0 updated_retain_activations.norm= 1720.0\n",
      "Topic 0 frozen_retain_activations.norm= 1720.0\n",
      " 19%|███████▉                                  | 57/300 [00:54<03:52,  1.05it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 512, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1360 | unlearn_loss: 1360 | retain_loss: 0.0188 | param_change: 1.353e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1664.0\n",
      "Topic 0 frozen_forget_activations.norm= 1664.0\n",
      "Topic 0 updated_retain_activations.norm= 1328.0\n",
      "Topic 0 frozen_retain_activations.norm= 1328.0\n",
      " 19%|████████                                  | 58/300 [00:55<04:16,  1.06s/it]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 247, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 904 | unlearn_loss: 904 | retain_loss: 0.01532 | param_change: 1.192e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 908.0\n",
      "Topic 0 frozen_forget_activations.norm= 908.0\n",
      "Topic 0 updated_retain_activations.norm= 1464.0\n",
      "Topic 0 frozen_retain_activations.norm= 1464.0\n",
      " 20%|████████▎                                 | 59/300 [00:56<04:05,  1.02s/it]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 259, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1256 | unlearn_loss: 1256 | retain_loss: 0.01538 | param_change: 1.538e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1264.0\n",
      "Topic 0 frozen_forget_activations.norm= 1264.0\n",
      "Topic 0 updated_retain_activations.norm= 748.0\n",
      "Topic 0 frozen_retain_activations.norm= 748.0\n",
      " 20%|████████▍                                 | 60/300 [00:57<03:45,  1.07it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 260, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1112 | unlearn_loss: 1112 | retain_loss: 0.01385 | param_change: 1.323e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1152.0\n",
      "Topic 0 frozen_forget_activations.norm= 1152.0\n",
      "Topic 0 updated_retain_activations.norm= 1680.0\n",
      "Topic 0 frozen_retain_activations.norm= 1680.0\n",
      " 20%|████████▌                                 | 61/300 [00:58<03:43,  1.07it/s]loss: 1416 | unlearn_loss: 1416 | retain_loss: 0.01178 | param_change: 1.574e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1464.0\n",
      "Topic 0 frozen_forget_activations.norm= 1464.0\n",
      "Topic 0 updated_retain_activations.norm= 1904.0\n",
      "Topic 0 frozen_retain_activations.norm= 1904.0\n",
      " 21%|████████▋                                 | 62/300 [00:59<03:55,  1.01it/s]loss: 1440 | unlearn_loss: 1440 | retain_loss: 0.01508 | param_change: 1.585e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1592.0\n",
      "Topic 0 frozen_forget_activations.norm= 1592.0\n",
      "Topic 0 updated_retain_activations.norm= 1280.0\n",
      "Topic 0 frozen_retain_activations.norm= 1280.0\n",
      " 21%|████████▊                                 | 63/300 [01:00<03:52,  1.02it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 386, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1416 | unlearn_loss: 1416 | retain_loss: 0.01532 | param_change: 1.371e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1576.0\n",
      "Topic 0 frozen_forget_activations.norm= 1576.0\n",
      "Topic 0 updated_retain_activations.norm= 1020.0\n",
      "Topic 0 frozen_retain_activations.norm= 1020.0\n",
      " 21%|████████▉                                 | 64/300 [01:01<03:56,  1.00s/it]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 312, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1368 | unlearn_loss: 1368 | retain_loss: 0.01508 | param_change: 1.454e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1432.0\n",
      "Topic 0 frozen_forget_activations.norm= 1432.0\n",
      "Topic 0 updated_retain_activations.norm= 1720.0\n",
      "Topic 0 frozen_retain_activations.norm= 1720.0\n",
      " 22%|█████████                                 | 65/300 [01:02<03:57,  1.01s/it]loss: 1184 | unlearn_loss: 1184 | retain_loss: 0.01892 | param_change: 1.508e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1208.0\n",
      "Topic 0 frozen_forget_activations.norm= 1208.0\n",
      "Topic 0 updated_retain_activations.norm= 1304.0\n",
      "Topic 0 frozen_retain_activations.norm= 1304.0\n",
      " 22%|█████████▏                                | 66/300 [01:03<03:35,  1.09it/s]loss: 1072 | unlearn_loss: 1072 | retain_loss: 0.02356 | param_change: 1.472e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1120.0\n",
      "Topic 0 frozen_forget_activations.norm= 1120.0\n",
      "Topic 0 updated_retain_activations.norm= 1240.0\n",
      "Topic 0 frozen_retain_activations.norm= 1240.0\n",
      " 22%|█████████▍                                | 67/300 [01:03<03:30,  1.10it/s]loss: 1416 | unlearn_loss: 1416 | retain_loss: 0.021 | param_change: 1.305e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1592.0\n",
      "Topic 0 frozen_forget_activations.norm= 1592.0\n",
      "Topic 0 updated_retain_activations.norm= 1264.0\n",
      "Topic 0 frozen_retain_activations.norm= 1264.0\n",
      " 23%|█████████▌                                | 68/300 [01:04<03:30,  1.10it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 323, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1456 | unlearn_loss: 1456 | retain_loss: 0.01544 | param_change: 1.383e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1552.0\n",
      "Topic 0 frozen_forget_activations.norm= 1552.0\n",
      "Topic 0 updated_retain_activations.norm= 1712.0\n",
      "Topic 0 frozen_retain_activations.norm= 1712.0\n",
      " 23%|█████████▋                                | 69/300 [01:05<03:40,  1.05it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 325, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1344 | unlearn_loss: 1344 | retain_loss: 0.01721 | param_change: 1.436e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1432.0\n",
      "Topic 0 frozen_forget_activations.norm= 1432.0\n",
      "Topic 0 updated_retain_activations.norm= 1520.0\n",
      "Topic 0 frozen_retain_activations.norm= 1520.0\n",
      " 23%|█████████▊                                | 70/300 [01:06<03:39,  1.05it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 316, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1144 | unlearn_loss: 1144 | retain_loss: 0.01611 | param_change: 1.466e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1168.0\n",
      "Topic 0 frozen_forget_activations.norm= 1168.0\n",
      "Topic 0 updated_retain_activations.norm= 1440.0\n",
      "Topic 0 frozen_retain_activations.norm= 1440.0\n",
      " 24%|█████████▉                                | 71/300 [01:07<03:46,  1.01it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 252, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1416 | unlearn_loss: 1416 | retain_loss: 0.01636 | param_change: 1.425e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1472.0\n",
      "Topic 0 frozen_forget_activations.norm= 1472.0\n",
      "Topic 0 updated_retain_activations.norm= 1664.0\n",
      "Topic 0 frozen_retain_activations.norm= 1664.0\n",
      " 24%|██████████                                | 72/300 [01:08<03:42,  1.03it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 276, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1064 | unlearn_loss: 1064 | retain_loss: 0.01917 | param_change: 1.156e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1120.0\n",
      "Topic 0 frozen_forget_activations.norm= 1120.0\n",
      "Topic 0 updated_retain_activations.norm= 1352.0\n",
      "Topic 0 frozen_retain_activations.norm= 1352.0\n",
      " 24%|██████████▏                               | 73/300 [01:09<03:35,  1.05it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 294, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1280 | unlearn_loss: 1280 | retain_loss: 0.01758 | param_change: 1.389e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1336.0\n",
      "Topic 0 frozen_forget_activations.norm= 1336.0\n",
      "Topic 0 updated_retain_activations.norm= 1568.0\n",
      "Topic 0 frozen_retain_activations.norm= 1568.0\n",
      " 25%|██████████▎                               | 74/300 [01:10<03:44,  1.01it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 334, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1280 | unlearn_loss: 1280 | retain_loss: 0.01758 | param_change: 1.431e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1368.0\n",
      "Topic 0 frozen_forget_activations.norm= 1368.0\n",
      "Topic 0 updated_retain_activations.norm= 1728.0\n",
      "Topic 0 frozen_retain_activations.norm= 1728.0\n",
      " 25%|██████████▌                               | 75/300 [01:11<03:42,  1.01it/s]loss: 972 | unlearn_loss: 972 | retain_loss: 0.01709 | param_change: 1.538e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 940.0\n",
      "Topic 0 frozen_forget_activations.norm= 940.0\n",
      "Topic 0 updated_retain_activations.norm= 1600.0\n",
      "Topic 0 frozen_retain_activations.norm= 1600.0\n",
      " 25%|██████████▋                               | 76/300 [01:12<03:39,  1.02it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 315, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1472 | unlearn_loss: 1472 | retain_loss: 0.02185 | param_change: 1.311e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1584.0\n",
      "Topic 0 frozen_forget_activations.norm= 1584.0\n",
      "Topic 0 updated_retain_activations.norm= 1032.0\n",
      "Topic 0 frozen_retain_activations.norm= 1032.0\n",
      " 26%|██████████▊                               | 77/300 [01:13<03:35,  1.04it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 313, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1192 | unlearn_loss: 1192 | retain_loss: 0.02075 | param_change: 1.305e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1256.0\n",
      "Topic 0 frozen_forget_activations.norm= 1256.0\n",
      "Topic 0 updated_retain_activations.norm= 1096.0\n",
      "Topic 0 frozen_retain_activations.norm= 1096.0\n",
      " 26%|██████████▉                               | 78/300 [01:14<03:27,  1.07it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 201, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 656 | unlearn_loss: 656 | retain_loss: 0.02161 | param_change: 1.335e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 632.0\n",
      "Topic 0 frozen_forget_activations.norm= 632.0\n",
      "Topic 0 updated_retain_activations.norm= 968.0\n",
      "Topic 0 frozen_retain_activations.norm= 968.0\n",
      " 26%|███████████                               | 79/300 [01:15<03:07,  1.18it/s]loss: 1240 | unlearn_loss: 1240 | retain_loss: 0.01733 | param_change: 1.621e-05\n",
      "unlearn_cosine_sim=0.99609375\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1520.0\n",
      "Topic 0 frozen_forget_activations.norm= 1520.0\n",
      "Topic 0 updated_retain_activations.norm= 1352.0\n",
      "Topic 0 frozen_retain_activations.norm= 1352.0\n",
      " 27%|███████████▏                              | 80/300 [01:16<03:36,  1.02it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 226, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1144 | unlearn_loss: 1144 | retain_loss: 0.01746 | param_change: 1.174e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1168.0\n",
      "Topic 0 frozen_forget_activations.norm= 1168.0\n",
      "Topic 0 updated_retain_activations.norm= 1360.0\n",
      "Topic 0 frozen_retain_activations.norm= 1360.0\n",
      " 27%|███████████▎                              | 81/300 [01:17<03:22,  1.08it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 292, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1152 | unlearn_loss: 1152 | retain_loss: 0.01855 | param_change: 1.538e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1136.0\n",
      "Topic 0 frozen_forget_activations.norm= 1136.0\n",
      "Topic 0 updated_retain_activations.norm= 1424.0\n",
      "Topic 0 frozen_retain_activations.norm= 1424.0\n",
      " 27%|███████████▍                              | 82/300 [01:18<03:19,  1.09it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 246, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1232 | unlearn_loss: 1232 | retain_loss: 0.02087 | param_change: 1.538e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1200.0\n",
      "Topic 0 frozen_forget_activations.norm= 1200.0\n",
      "Topic 0 updated_retain_activations.norm= 1168.0\n",
      "Topic 0 frozen_retain_activations.norm= 1168.0\n",
      " 28%|███████████▌                              | 83/300 [01:19<03:09,  1.15it/s]loss: 1064 | unlearn_loss: 1064 | retain_loss: 0.01917 | param_change: 1.729e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1056.0\n",
      "Topic 0 frozen_forget_activations.norm= 1056.0\n",
      "Topic 0 updated_retain_activations.norm= 1544.0\n",
      "Topic 0 frozen_retain_activations.norm= 1544.0\n",
      " 28%|███████████▊                              | 84/300 [01:19<03:04,  1.17it/s]loss: 1256 | unlearn_loss: 1256 | retain_loss: 0.0199 | param_change: 1.258e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1336.0\n",
      "Topic 0 frozen_forget_activations.norm= 1336.0\n",
      "Topic 0 updated_retain_activations.norm= 880.0\n",
      "Topic 0 frozen_retain_activations.norm= 880.0\n",
      " 28%|███████████▉                              | 85/300 [01:20<03:02,  1.18it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 339, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1544 | unlearn_loss: 1544 | retain_loss: 0.0238 | param_change: 2.003e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1760.0\n",
      "Topic 0 frozen_forget_activations.norm= 1760.0\n",
      "Topic 0 updated_retain_activations.norm= 984.0\n",
      "Topic 0 frozen_retain_activations.norm= 984.0\n",
      " 29%|████████████                              | 86/300 [01:21<03:08,  1.14it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 255, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1056 | unlearn_loss: 1056 | retain_loss: 0.02747 | param_change: 2.742e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1064.0\n",
      "Topic 0 frozen_forget_activations.norm= 1064.0\n",
      "Topic 0 updated_retain_activations.norm= 948.0\n",
      "Topic 0 frozen_retain_activations.norm= 948.0\n",
      " 29%|████████████▏                             | 87/300 [01:22<03:00,  1.18it/s]loss: 1224 | unlearn_loss: 1224 | retain_loss: 0.02075 | param_change: 1.407e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1256.0\n",
      "Topic 0 frozen_forget_activations.norm= 1256.0\n",
      "Topic 0 updated_retain_activations.norm= 780.0\n",
      "Topic 0 frozen_retain_activations.norm= 780.0\n",
      " 29%|████████████▎                             | 88/300 [01:23<02:59,  1.18it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 286, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 864 | unlearn_loss: 864 | retain_loss: 0.02258 | param_change: 1.371e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 852.0\n",
      "Topic 0 frozen_forget_activations.norm= 852.0\n",
      "Topic 0 updated_retain_activations.norm= 1424.0\n",
      "Topic 0 frozen_retain_activations.norm= 1424.0\n",
      " 30%|████████████▍                             | 89/300 [01:24<03:01,  1.16it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 300, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1184 | unlearn_loss: 1184 | retain_loss: 0.0199 | param_change: 1.597e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1232.0\n",
      "Topic 0 frozen_forget_activations.norm= 1232.0\n",
      "Topic 0 updated_retain_activations.norm= 1672.0\n",
      "Topic 0 frozen_retain_activations.norm= 1672.0\n",
      " 30%|████████████▌                             | 90/300 [01:25<03:15,  1.07it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 251, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1144 | unlearn_loss: 1144 | retain_loss: 0.02063 | param_change: 1.538e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1112.0\n",
      "Topic 0 frozen_forget_activations.norm= 1112.0\n",
      "Topic 0 updated_retain_activations.norm= 1208.0\n",
      "Topic 0 frozen_retain_activations.norm= 1208.0\n",
      " 30%|████████████▋                             | 91/300 [01:26<03:06,  1.12it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 237, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1016 | unlearn_loss: 1016 | retain_loss: 0.0238 | param_change: 1.454e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 992.0\n",
      "Topic 0 frozen_forget_activations.norm= 992.0\n",
      "Topic 0 updated_retain_activations.norm= 1004.0\n",
      "Topic 0 frozen_retain_activations.norm= 1004.0\n",
      " 31%|████████████▉                             | 92/300 [01:26<02:53,  1.20it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 196, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1120 | unlearn_loss: 1120 | retain_loss: 0.02051 | param_change: 1.371e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1128.0\n",
      "Topic 0 frozen_forget_activations.norm= 1128.0\n",
      "Topic 0 updated_retain_activations.norm= 1432.0\n",
      "Topic 0 frozen_retain_activations.norm= 1432.0\n",
      " 31%|█████████████                             | 93/300 [01:27<02:49,  1.22it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 240, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1048 | unlearn_loss: 1048 | retain_loss: 0.02429 | param_change: 1.669e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1016.0\n",
      "Topic 0 frozen_forget_activations.norm= 1016.0\n",
      "Topic 0 updated_retain_activations.norm= 1664.0\n",
      "Topic 0 frozen_retain_activations.norm= 1664.0\n",
      " 31%|█████████████▏                            | 94/300 [01:28<02:55,  1.18it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 232, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1456 | unlearn_loss: 1456 | retain_loss: 0.0177 | param_change: 1.693e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1480.0\n",
      "Topic 0 frozen_forget_activations.norm= 1480.0\n",
      "Topic 0 updated_retain_activations.norm= 1584.0\n",
      "Topic 0 frozen_retain_activations.norm= 1584.0\n",
      " 32%|█████████████▎                            | 95/300 [01:29<02:50,  1.20it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 222, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 764 | unlearn_loss: 764 | retain_loss: 0.02258 | param_change: 1.448e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 740.0\n",
      "Topic 0 frozen_forget_activations.norm= 740.0\n",
      "Topic 0 updated_retain_activations.norm= 1264.0\n",
      "Topic 0 frozen_retain_activations.norm= 1264.0\n",
      " 32%|█████████████▍                            | 96/300 [01:29<02:42,  1.26it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 287, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1432 | unlearn_loss: 1432 | retain_loss: 0.01721 | param_change: 1.55e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1488.0\n",
      "Topic 0 frozen_forget_activations.norm= 1488.0\n",
      "Topic 0 updated_retain_activations.norm= 1792.0\n",
      "Topic 0 frozen_retain_activations.norm= 1792.0\n",
      " 32%|█████████████▌                            | 97/300 [01:30<02:54,  1.16it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 326, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1392 | unlearn_loss: 1392 | retain_loss: 0.02234 | param_change: 1.365e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1568.0\n",
      "Topic 0 frozen_forget_activations.norm= 1568.0\n",
      "Topic 0 updated_retain_activations.norm= 1416.0\n",
      "Topic 0 frozen_retain_activations.norm= 1416.0\n",
      " 33%|█████████████▋                            | 98/300 [01:32<03:08,  1.07it/s]loss: 1448 | unlearn_loss: 1448 | retain_loss: 0.02686 | param_change: 1.192e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1840.0\n",
      "Topic 0 frozen_forget_activations.norm= 1840.0\n",
      "Topic 0 updated_retain_activations.norm= 1240.0\n",
      "Topic 0 frozen_retain_activations.norm= 1240.0\n",
      " 33%|█████████████▊                            | 99/300 [01:33<03:26,  1.03s/it]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 421, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1416 | unlearn_loss: 1416 | retain_loss: 0.02588 | param_change: 1.162e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1680.0\n",
      "Topic 0 frozen_forget_activations.norm= 1680.0\n",
      "Topic 0 updated_retain_activations.norm= 1208.0\n",
      "Topic 0 frozen_retain_activations.norm= 1216.0\n",
      " 33%|█████████████▋                           | 100/300 [01:34<03:37,  1.09s/it]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 330, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1360 | unlearn_loss: 1360 | retain_loss: 0.021 | param_change: 1.419e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1448.0\n",
      "Topic 0 frozen_forget_activations.norm= 1448.0\n",
      "Topic 0 updated_retain_activations.norm= 1272.0\n",
      "Topic 0 frozen_retain_activations.norm= 1272.0\n",
      " 34%|█████████████▊                           | 101/300 [01:35<03:29,  1.05s/it]loss: 1360 | unlearn_loss: 1360 | retain_loss: 0.01929 | param_change: 1.442e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1456.0\n",
      "Topic 0 frozen_forget_activations.norm= 1456.0\n",
      "Topic 0 updated_retain_activations.norm= 1304.0\n",
      "Topic 0 frozen_retain_activations.norm= 1304.0\n",
      " 34%|█████████████▉                           | 102/300 [01:36<03:23,  1.03s/it]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 281, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1288 | unlearn_loss: 1288 | retain_loss: 0.03027 | param_change: 1.705e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1304.0\n",
      "Topic 0 frozen_forget_activations.norm= 1304.0\n",
      "Topic 0 updated_retain_activations.norm= 1040.0\n",
      "Topic 0 frozen_retain_activations.norm= 1040.0\n",
      " 34%|██████████████                           | 103/300 [01:37<03:08,  1.05it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 214, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1376 | unlearn_loss: 1376 | retain_loss: 0.02161 | param_change: 1.621e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1416.0\n",
      "Topic 0 frozen_forget_activations.norm= 1416.0\n",
      "Topic 0 updated_retain_activations.norm= 1384.0\n",
      "Topic 0 frozen_retain_activations.norm= 1384.0\n",
      " 35%|██████████████▏                          | 104/300 [01:38<02:55,  1.12it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 254, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1160 | unlearn_loss: 1160 | retain_loss: 0.02344 | param_change: 1.562e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1152.0\n",
      "Topic 0 frozen_forget_activations.norm= 1152.0\n",
      "Topic 0 updated_retain_activations.norm= 1128.0\n",
      "Topic 0 frozen_retain_activations.norm= 1128.0\n",
      " 35%|██████████████▎                          | 105/300 [01:38<02:47,  1.17it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 215, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1160 | unlearn_loss: 1160 | retain_loss: 0.02441 | param_change: 1.371e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1152.0\n",
      "Topic 0 frozen_forget_activations.norm= 1152.0\n",
      "Topic 0 updated_retain_activations.norm= 1416.0\n",
      "Topic 0 frozen_retain_activations.norm= 1416.0\n",
      " 35%|██████████████▍                          | 106/300 [01:39<02:49,  1.15it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 436, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1456 | unlearn_loss: 1456 | retain_loss: 0.02771 | param_change: 1.258e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1768.0\n",
      "Topic 0 frozen_forget_activations.norm= 1768.0\n",
      "Topic 0 updated_retain_activations.norm= 1128.0\n",
      "Topic 0 frozen_retain_activations.norm= 1128.0\n",
      " 36%|██████████████▌                          | 107/300 [01:40<03:11,  1.01it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 293, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1080 | unlearn_loss: 1080 | retain_loss: 0.02222 | param_change: 1.74e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1064.0\n",
      "Topic 0 frozen_forget_activations.norm= 1064.0\n",
      "Topic 0 updated_retain_activations.norm= 720.0\n",
      "Topic 0 frozen_retain_activations.norm= 720.0\n",
      " 36%|██████████████▊                          | 108/300 [01:41<03:02,  1.05it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 308, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1416 | unlearn_loss: 1416 | retain_loss: 0.02197 | param_change: 1.496e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1504.0\n",
      "Topic 0 frozen_forget_activations.norm= 1504.0\n",
      "Topic 0 updated_retain_activations.norm= 1488.0\n",
      "Topic 0 frozen_retain_activations.norm= 1488.0\n",
      " 36%|██████████████▉                          | 109/300 [01:42<03:08,  1.01it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 304, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1488 | unlearn_loss: 1488 | retain_loss: 0.02026 | param_change: 1.645e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1592.0\n",
      "Topic 0 frozen_forget_activations.norm= 1592.0\n",
      "Topic 0 updated_retain_activations.norm= 1464.0\n",
      "Topic 0 frozen_retain_activations.norm= 1464.0\n",
      " 37%|███████████████                          | 110/300 [01:43<03:03,  1.04it/s]loss: 720 | unlearn_loss: 720 | retain_loss: 0.0293 | param_change: 1.228e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 720.0\n",
      "Topic 0 frozen_forget_activations.norm= 720.0\n",
      "Topic 0 updated_retain_activations.norm= 1480.0\n",
      "Topic 0 frozen_retain_activations.norm= 1480.0\n",
      " 37%|███████████████▏                         | 111/300 [01:44<02:54,  1.09it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 256, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1048 | unlearn_loss: 1048 | retain_loss: 0.01648 | param_change: 1.436e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1064.0\n",
      "Topic 0 frozen_forget_activations.norm= 1064.0\n",
      "Topic 0 updated_retain_activations.norm= 1888.0\n",
      "Topic 0 frozen_retain_activations.norm= 1888.0\n",
      " 37%|███████████████▎                         | 112/300 [01:45<03:14,  1.03s/it]loss: 1200 | unlearn_loss: 1200 | retain_loss: 0.03101 | param_change: 1.454e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1232.0\n",
      "Topic 0 frozen_forget_activations.norm= 1232.0\n",
      "Topic 0 updated_retain_activations.norm= 968.0\n",
      "Topic 0 frozen_retain_activations.norm= 968.0\n",
      " 38%|███████████████▍                         | 113/300 [01:46<02:54,  1.07it/s]loss: 1272 | unlearn_loss: 1272 | retain_loss: 0.02832 | param_change: 1.669e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1288.0\n",
      "Topic 0 frozen_forget_activations.norm= 1288.0\n",
      "Topic 0 updated_retain_activations.norm= 1088.0\n",
      "Topic 0 frozen_retain_activations.norm= 1088.0\n",
      " 38%|███████████████▌                         | 114/300 [01:47<02:44,  1.13it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 375, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1528 | unlearn_loss: 1528 | retain_loss: 0.02271 | param_change: 1.991e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1832.0\n",
      "Topic 0 frozen_forget_activations.norm= 1832.0\n",
      "Topic 0 updated_retain_activations.norm= 1400.0\n",
      "Topic 0 frozen_retain_activations.norm= 1400.0\n",
      " 38%|███████████████▋                         | 115/300 [01:48<02:51,  1.08it/s]loss: 1216 | unlearn_loss: 1216 | retain_loss: 0.02258 | param_change: 1.419e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1240.0\n",
      "Topic 0 frozen_forget_activations.norm= 1240.0\n",
      "Topic 0 updated_retain_activations.norm= 1080.0\n",
      "Topic 0 frozen_retain_activations.norm= 1080.0\n",
      " 39%|███████████████▊                         | 116/300 [01:49<02:44,  1.12it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 346, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1408 | unlearn_loss: 1408 | retain_loss: 0.02173 | param_change: 1.436e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1560.0\n",
      "Topic 0 frozen_forget_activations.norm= 1560.0\n",
      "Topic 0 updated_retain_activations.norm= 1248.0\n",
      "Topic 0 frozen_retain_activations.norm= 1248.0\n",
      " 39%|███████████████▉                         | 117/300 [01:50<02:48,  1.09it/s]loss: 1280 | unlearn_loss: 1280 | retain_loss: 0.03003 | param_change: 1.419e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1352.0\n",
      "Topic 0 frozen_forget_activations.norm= 1352.0\n",
      "Topic 0 updated_retain_activations.norm= 1504.0\n",
      "Topic 0 frozen_retain_activations.norm= 1504.0\n",
      " 39%|████████████████▏                        | 118/300 [01:51<02:50,  1.07it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 345, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1488 | unlearn_loss: 1488 | retain_loss: 0.02759 | param_change: 1.478e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1688.0\n",
      "Topic 0 frozen_forget_activations.norm= 1688.0\n",
      "Topic 0 updated_retain_activations.norm= 1232.0\n",
      "Topic 0 frozen_retain_activations.norm= 1232.0\n",
      " 40%|████████████████▎                        | 119/300 [01:52<02:47,  1.08it/s]loss: 684 | unlearn_loss: 684 | retain_loss: 0.03198 | param_change: 1.645e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 652.0\n",
      "Topic 0 frozen_forget_activations.norm= 652.0\n",
      "Topic 0 updated_retain_activations.norm= 1256.0\n",
      "Topic 0 frozen_retain_activations.norm= 1256.0\n",
      " 40%|████████████████▍                        | 120/300 [01:52<02:36,  1.15it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 270, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1152 | unlearn_loss: 1152 | retain_loss: 0.02234 | param_change: 1.538e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1160.0\n",
      "Topic 0 frozen_forget_activations.norm= 1160.0\n",
      "Topic 0 updated_retain_activations.norm= 1512.0\n",
      "Topic 0 frozen_retain_activations.norm= 1512.0\n",
      " 40%|████████████████▌                        | 121/300 [01:53<02:36,  1.14it/s]loss: 1520 | unlearn_loss: 1520 | retain_loss: 0.02832 | param_change: 1.907e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1664.0\n",
      "Topic 0 frozen_forget_activations.norm= 1664.0\n",
      "Topic 0 updated_retain_activations.norm= 868.0\n",
      "Topic 0 frozen_retain_activations.norm= 868.0\n",
      " 41%|████████████████▋                        | 122/300 [01:54<02:37,  1.13it/s]loss: 1096 | unlearn_loss: 1096 | retain_loss: 0.0293 | param_change: 1.389e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1096.0\n",
      "Topic 0 frozen_forget_activations.norm= 1096.0\n",
      "Topic 0 updated_retain_activations.norm= 980.0\n",
      "Topic 0 frozen_retain_activations.norm= 980.0\n",
      " 41%|████████████████▊                        | 123/300 [01:55<02:31,  1.17it/s]loss: 568 | unlearn_loss: 568 | retain_loss: 0.02332 | param_change: 1.46e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 544.0\n",
      "Topic 0 frozen_forget_activations.norm= 544.0\n",
      "Topic 0 updated_retain_activations.norm= 880.0\n",
      "Topic 0 frozen_retain_activations.norm= 880.0\n",
      " 41%|████████████████▉                        | 124/300 [01:56<02:21,  1.24it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 381, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1320 | unlearn_loss: 1320 | retain_loss: 0.02527 | param_change: 1.955e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1464.0\n",
      "Topic 0 frozen_forget_activations.norm= 1464.0\n",
      "Topic 0 updated_retain_activations.norm= 1440.0\n",
      "Topic 0 frozen_retain_activations.norm= 1440.0\n",
      " 42%|█████████████████                        | 125/300 [01:57<02:32,  1.15it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 249, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 872 | unlearn_loss: 872 | retain_loss: 0.02197 | param_change: 1.216e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 884.0\n",
      "Topic 0 frozen_forget_activations.norm= 884.0\n",
      "Topic 0 updated_retain_activations.norm= 1600.0\n",
      "Topic 0 frozen_retain_activations.norm= 1600.0\n",
      " 42%|█████████████████▏                       | 126/300 [01:58<02:34,  1.12it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 205, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 820 | unlearn_loss: 820 | retain_loss: 0.02405 | param_change: 1.419e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 796.0\n",
      "Topic 0 frozen_forget_activations.norm= 796.0\n",
      "Topic 0 updated_retain_activations.norm= 1280.0\n",
      "Topic 0 frozen_retain_activations.norm= 1280.0\n",
      " 42%|█████████████████▎                       | 127/300 [01:58<02:28,  1.17it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 288, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1336 | unlearn_loss: 1336 | retain_loss: 0.02271 | param_change: 1.311e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1440.0\n",
      "Topic 0 frozen_forget_activations.norm= 1440.0\n",
      "Topic 0 updated_retain_activations.norm= 1528.0\n",
      "Topic 0 frozen_retain_activations.norm= 1528.0\n",
      " 43%|█████████████████▍                       | 128/300 [01:59<02:35,  1.11it/s]loss: 1104 | unlearn_loss: 1104 | retain_loss: 0.02515 | param_change: 1.24e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1160.0\n",
      "Topic 0 frozen_forget_activations.norm= 1160.0\n",
      "Topic 0 updated_retain_activations.norm= 1160.0\n",
      "Topic 0 frozen_retain_activations.norm= 1160.0\n",
      " 43%|█████████████████▋                       | 129/300 [02:00<02:28,  1.15it/s]loss: 1112 | unlearn_loss: 1112 | retain_loss: 0.02271 | param_change: 1.365e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1152.0\n",
      "Topic 0 frozen_forget_activations.norm= 1152.0\n",
      "Topic 0 updated_retain_activations.norm= 1576.0\n",
      "Topic 0 frozen_retain_activations.norm= 1576.0\n",
      " 43%|█████████████████▊                       | 130/300 [02:01<02:28,  1.14it/s]loss: 1176 | unlearn_loss: 1176 | retain_loss: 0.03394 | param_change: 1.55e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1224.0\n",
      "Topic 0 frozen_forget_activations.norm= 1224.0\n",
      "Topic 0 updated_retain_activations.norm= 1048.0\n",
      "Topic 0 frozen_retain_activations.norm= 1048.0\n",
      " 44%|█████████████████▉                       | 131/300 [02:02<02:25,  1.16it/s]loss: 1512 | unlearn_loss: 1512 | retain_loss: 0.02612 | param_change: 1.377e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1648.0\n",
      "Topic 0 frozen_forget_activations.norm= 1648.0\n",
      "Topic 0 updated_retain_activations.norm= 1504.0\n",
      "Topic 0 frozen_retain_activations.norm= 1504.0\n",
      " 44%|██████████████████                       | 132/300 [02:03<02:32,  1.10it/s]loss: 1400 | unlearn_loss: 1400 | retain_loss: 0.02295 | param_change: 1.431e-05\n",
      "unlearn_cosine_sim=0.99609375\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1760.0\n",
      "Topic 0 frozen_forget_activations.norm= 1760.0\n",
      "Topic 0 updated_retain_activations.norm= 1432.0\n",
      "Topic 0 frozen_retain_activations.norm= 1432.0\n",
      " 44%|██████████████████▏                      | 133/300 [02:04<02:53,  1.04s/it]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 272, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 916 | unlearn_loss: 916 | retain_loss: 0.02576 | param_change: 1.508e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 884.0\n",
      "Topic 0 frozen_forget_activations.norm= 884.0\n",
      "Topic 0 updated_retain_activations.norm= 872.0\n",
      "Topic 0 frozen_retain_activations.norm= 872.0\n",
      " 45%|██████████████████▎                      | 134/300 [02:05<02:43,  1.02it/s]loss: 1048 | unlearn_loss: 1048 | retain_loss: 0.03467 | param_change: 2.122e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1176.0\n",
      "Topic 0 frozen_forget_activations.norm= 1176.0\n",
      "Topic 0 updated_retain_activations.norm= 1184.0\n",
      "Topic 0 frozen_retain_activations.norm= 1184.0\n",
      " 45%|██████████████████▍                      | 135/300 [02:06<02:39,  1.03it/s]loss: 1472 | unlearn_loss: 1472 | retain_loss: 0.0293 | param_change: 1.24e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1640.0\n",
      "Topic 0 frozen_forget_activations.norm= 1640.0\n",
      "Topic 0 updated_retain_activations.norm= 1408.0\n",
      "Topic 0 frozen_retain_activations.norm= 1408.0\n",
      " 45%|██████████████████▌                      | 136/300 [02:07<02:41,  1.01it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 218, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1248 | unlearn_loss: 1248 | retain_loss: 0.02747 | param_change: 1.341e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1248.0\n",
      "Topic 0 frozen_forget_activations.norm= 1248.0\n",
      "Topic 0 updated_retain_activations.norm= 1232.0\n",
      "Topic 0 frozen_retain_activations.norm= 1232.0\n",
      " 46%|██████████████████▋                      | 137/300 [02:08<02:30,  1.08it/s]loss: 1312 | unlearn_loss: 1312 | retain_loss: 0.03125 | param_change: 1.55e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1312.0\n",
      "Topic 0 frozen_forget_activations.norm= 1312.0\n",
      "Topic 0 updated_retain_activations.norm= 1752.0\n",
      "Topic 0 frozen_retain_activations.norm= 1752.0\n",
      " 46%|██████████████████▊                      | 138/300 [02:09<02:32,  1.06it/s]loss: 996 | unlearn_loss: 996 | retain_loss: 0.02356 | param_change: 1.705e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 976.0\n",
      "Topic 0 frozen_forget_activations.norm= 976.0\n",
      "Topic 0 updated_retain_activations.norm= 1480.0\n",
      "Topic 0 frozen_retain_activations.norm= 1480.0\n",
      " 46%|██████████████████▉                      | 139/300 [02:10<02:27,  1.09it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 212, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 912 | unlearn_loss: 912 | retain_loss: 0.02991 | param_change: 1.52e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 896.0\n",
      "Topic 0 frozen_forget_activations.norm= 896.0\n",
      "Topic 0 updated_retain_activations.norm= 1256.0\n",
      "Topic 0 frozen_retain_activations.norm= 1256.0\n",
      " 47%|███████████████████▏                     | 140/300 [02:11<02:26,  1.10it/s]loss: 988 | unlearn_loss: 988 | retain_loss: 0.02747 | param_change: 1.347e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 984.0\n",
      "Topic 0 frozen_forget_activations.norm= 984.0\n",
      "Topic 0 updated_retain_activations.norm= 1248.0\n",
      "Topic 0 frozen_retain_activations.norm= 1248.0\n",
      " 47%|███████████████████▎                     | 141/300 [02:11<02:20,  1.13it/s]loss: 1240 | unlearn_loss: 1240 | retain_loss: 0.02002 | param_change: 1.669e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1248.0\n",
      "Topic 0 frozen_forget_activations.norm= 1248.0\n",
      "Topic 0 updated_retain_activations.norm= 1864.0\n",
      "Topic 0 frozen_retain_activations.norm= 1864.0\n",
      " 47%|███████████████████▍                     | 142/300 [02:12<02:17,  1.15it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 203, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1096 | unlearn_loss: 1096 | retain_loss: 0.03271 | param_change: 1.478e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1064.0\n",
      "Topic 0 frozen_forget_activations.norm= 1064.0\n",
      "Topic 0 updated_retain_activations.norm= 1120.0\n",
      "Topic 0 frozen_retain_activations.norm= 1120.0\n",
      " 48%|███████████████████▌                     | 143/300 [02:13<02:07,  1.23it/s]loss: 932 | unlearn_loss: 932 | retain_loss: 0.02661 | param_change: 1.46e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 940.0\n",
      "Topic 0 frozen_forget_activations.norm= 940.0\n",
      "Topic 0 updated_retain_activations.norm= 856.0\n",
      "Topic 0 frozen_retain_activations.norm= 856.0\n",
      " 48%|███████████████████▋                     | 144/300 [02:14<02:08,  1.22it/s]loss: 716 | unlearn_loss: 716 | retain_loss: 0.021 | param_change: 1.454e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=0.99609375\n",
      "Topic 0 updated_forget_activations.norm= 672.0\n",
      "Topic 0 frozen_forget_activations.norm= 672.0\n",
      "Topic 0 updated_retain_activations.norm= 1632.0\n",
      "Topic 0 frozen_retain_activations.norm= 1632.0\n",
      " 48%|███████████████████▊                     | 145/300 [02:15<02:25,  1.07it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 406, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1472 | unlearn_loss: 1472 | retain_loss: 0.02576 | param_change: 1.144e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1784.0\n",
      "Topic 0 frozen_forget_activations.norm= 1784.0\n",
      "Topic 0 updated_retain_activations.norm= 1648.0\n",
      "Topic 0 frozen_retain_activations.norm= 1648.0\n",
      " 49%|███████████████████▉                     | 146/300 [02:16<02:37,  1.02s/it]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 365, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1448 | unlearn_loss: 1448 | retain_loss: 0.03223 | param_change: 1.454e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1648.0\n",
      "Topic 0 frozen_forget_activations.norm= 1648.0\n",
      "Topic 0 updated_retain_activations.norm= 1120.0\n",
      "Topic 0 frozen_retain_activations.norm= 1120.0\n",
      " 49%|████████████████████                     | 147/300 [02:17<02:32,  1.00it/s]loss: 1432 | unlearn_loss: 1432 | retain_loss: 0.02673 | param_change: 1.448e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1536.0\n",
      "Topic 0 frozen_forget_activations.norm= 1536.0\n",
      "Topic 0 updated_retain_activations.norm= 1328.0\n",
      "Topic 0 frozen_retain_activations.norm= 1328.0\n",
      " 49%|████████████████████▏                    | 148/300 [02:18<02:30,  1.01it/s]loss: 1328 | unlearn_loss: 1328 | retain_loss: 0.03015 | param_change: 1.502e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1368.0\n",
      "Topic 0 frozen_forget_activations.norm= 1368.0\n",
      "Topic 0 updated_retain_activations.norm= 1248.0\n",
      "Topic 0 frozen_retain_activations.norm= 1248.0\n",
      " 50%|████████████████████▎                    | 149/300 [02:19<02:36,  1.04s/it]loss: 744 | unlearn_loss: 744 | retain_loss: 0.02881 | param_change: 2.205e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 728.0\n",
      "Topic 0 frozen_forget_activations.norm= 728.0\n",
      "Topic 0 updated_retain_activations.norm= 1712.0\n",
      "Topic 0 frozen_retain_activations.norm= 1712.0\n",
      " 50%|████████████████████▌                    | 150/300 [02:20<02:36,  1.04s/it]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 360, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1544 | unlearn_loss: 1544 | retain_loss: 0.02844 | param_change: 1.371e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1808.0\n",
      "Topic 0 frozen_forget_activations.norm= 1808.0\n",
      "Topic 0 updated_retain_activations.norm= 1064.0\n",
      "Topic 0 frozen_retain_activations.norm= 1064.0\n",
      " 50%|████████████████████▋                    | 151/300 [02:21<02:33,  1.03s/it]loss: 1312 | unlearn_loss: 1312 | retain_loss: 0.03857 | param_change: 1.943e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1344.0\n",
      "Topic 0 frozen_forget_activations.norm= 1344.0\n",
      "Topic 0 updated_retain_activations.norm= 1512.0\n",
      "Topic 0 frozen_retain_activations.norm= 1512.0\n",
      " 51%|████████████████████▊                    | 152/300 [02:22<02:24,  1.02it/s]loss: 1096 | unlearn_loss: 1096 | retain_loss: 0.03345 | param_change: 1.574e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1056.0\n",
      "Topic 0 frozen_forget_activations.norm= 1056.0\n",
      "Topic 0 updated_retain_activations.norm= 1020.0\n",
      "Topic 0 frozen_retain_activations.norm= 1020.0\n",
      " 51%|████████████████████▉                    | 153/300 [02:23<02:19,  1.06it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 368, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1400 | unlearn_loss: 1400 | retain_loss: 0.03015 | param_change: 1.365e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1544.0\n",
      "Topic 0 frozen_forget_activations.norm= 1544.0\n",
      "Topic 0 updated_retain_activations.norm= 1488.0\n",
      "Topic 0 frozen_retain_activations.norm= 1488.0\n",
      " 51%|█████████████████████                    | 154/300 [02:24<02:29,  1.02s/it]loss: 1088 | unlearn_loss: 1088 | retain_loss: 0.05127 | param_change: 2.551e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1072.0\n",
      "Topic 0 frozen_forget_activations.norm= 1072.0\n",
      "Topic 0 updated_retain_activations.norm= 912.0\n",
      "Topic 0 frozen_retain_activations.norm= 912.0\n",
      " 52%|█████████████████████▏                   | 155/300 [02:25<02:22,  1.02it/s]loss: 1304 | unlearn_loss: 1304 | retain_loss: 0.02466 | param_change: 1.597e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1304.0\n",
      "Topic 0 frozen_forget_activations.norm= 1304.0\n",
      "Topic 0 updated_retain_activations.norm= 1680.0\n",
      "Topic 0 frozen_retain_activations.norm= 1680.0\n",
      " 52%|█████████████████████▎                   | 156/300 [02:26<02:23,  1.00it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 278, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1000 | unlearn_loss: 1000 | retain_loss: 0.02759 | param_change: 1.74e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 980.0\n",
      "Topic 0 frozen_forget_activations.norm= 980.0\n",
      "Topic 0 updated_retain_activations.norm= 1216.0\n",
      "Topic 0 frozen_retain_activations.norm= 1216.0\n",
      " 52%|█████████████████████▍                   | 157/300 [02:27<02:17,  1.04it/s]loss: 608 | unlearn_loss: 608 | retain_loss: 0.02966 | param_change: 1.222e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 584.0\n",
      "Topic 0 frozen_forget_activations.norm= 584.0\n",
      "Topic 0 updated_retain_activations.norm= 1280.0\n",
      "Topic 0 frozen_retain_activations.norm= 1280.0\n",
      " 53%|█████████████████████▌                   | 158/300 [02:28<02:08,  1.11it/s]loss: 1128 | unlearn_loss: 1128 | retain_loss: 0.03467 | param_change: 1.657e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1144.0\n",
      "Topic 0 frozen_forget_activations.norm= 1144.0\n",
      "Topic 0 updated_retain_activations.norm= 1256.0\n",
      "Topic 0 frozen_retain_activations.norm= 1256.0\n",
      " 53%|█████████████████████▋                   | 159/300 [02:29<02:06,  1.11it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 262, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1056 | unlearn_loss: 1056 | retain_loss: 0.03052 | param_change: 1.293e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1136.0\n",
      "Topic 0 frozen_forget_activations.norm= 1136.0\n",
      "Topic 0 updated_retain_activations.norm= 1600.0\n",
      "Topic 0 frozen_retain_activations.norm= 1600.0\n",
      " 53%|█████████████████████▊                   | 160/300 [02:30<02:08,  1.09it/s]loss: 1152 | unlearn_loss: 1152 | retain_loss: 0.02661 | param_change: 1.764e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1160.0\n",
      "Topic 0 frozen_forget_activations.norm= 1160.0\n",
      "Topic 0 updated_retain_activations.norm= 1744.0\n",
      "Topic 0 frozen_retain_activations.norm= 1744.0\n",
      " 54%|██████████████████████                   | 161/300 [02:31<02:14,  1.04it/s]loss: 900 | unlearn_loss: 900 | retain_loss: 0.03345 | param_change: 1.323e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 920.0\n",
      "Topic 0 frozen_forget_activations.norm= 920.0\n",
      "Topic 0 updated_retain_activations.norm= 1336.0\n",
      "Topic 0 frozen_retain_activations.norm= 1336.0\n",
      " 54%|██████████████████████▏                  | 162/300 [02:31<02:05,  1.10it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 289, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1020 | unlearn_loss: 1020 | retain_loss: 0.0354 | param_change: 1.365e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1024.0\n",
      "Topic 0 frozen_forget_activations.norm= 1024.0\n",
      "Topic 0 updated_retain_activations.norm= 1376.0\n",
      "Topic 0 frozen_retain_activations.norm= 1376.0\n",
      " 54%|██████████████████████▎                  | 163/300 [02:32<02:04,  1.10it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 239, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1056 | unlearn_loss: 1056 | retain_loss: 0.02747 | param_change: 1.705e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1088.0\n",
      "Topic 0 frozen_forget_activations.norm= 1088.0\n",
      "Topic 0 updated_retain_activations.norm= 1256.0\n",
      "Topic 0 frozen_retain_activations.norm= 1256.0\n",
      " 55%|██████████████████████▍                  | 164/300 [02:33<01:59,  1.14it/s]loss: 1456 | unlearn_loss: 1456 | retain_loss: 0.03882 | param_change: 3.6e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1880.0\n",
      "Topic 0 frozen_forget_activations.norm= 1880.0\n",
      "Topic 0 updated_retain_activations.norm= 1272.0\n",
      "Topic 0 frozen_retain_activations.norm= 1272.0\n",
      " 55%|██████████████████████▌                  | 165/300 [02:34<02:14,  1.00it/s]loss: 812 | unlearn_loss: 812 | retain_loss: 0.03516 | param_change: 1.496e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 772.0\n",
      "Topic 0 frozen_forget_activations.norm= 772.0\n",
      "Topic 0 updated_retain_activations.norm= 988.0\n",
      "Topic 0 frozen_retain_activations.norm= 988.0\n",
      " 55%|██████████████████████▋                  | 166/300 [02:35<02:03,  1.08it/s]loss: 1432 | unlearn_loss: 1432 | retain_loss: 0.03516 | param_change: 1.127e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1824.0\n",
      "Topic 0 frozen_forget_activations.norm= 1824.0\n",
      "Topic 0 updated_retain_activations.norm= 1192.0\n",
      "Topic 0 frozen_retain_activations.norm= 1192.0\n",
      " 56%|██████████████████████▊                  | 167/300 [02:36<02:15,  1.02s/it]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 274, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 992 | unlearn_loss: 992 | retain_loss: 0.03857 | param_change: 1.562e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 972.0\n",
      "Topic 0 frozen_forget_activations.norm= 972.0\n",
      "Topic 0 updated_retain_activations.norm= 1256.0\n",
      "Topic 0 frozen_retain_activations.norm= 1256.0\n",
      " 56%|██████████████████████▉                  | 168/300 [02:37<02:09,  1.02it/s]loss: 1320 | unlearn_loss: 1320 | retain_loss: 0.03296 | param_change: 1.496e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1328.0\n",
      "Topic 0 frozen_forget_activations.norm= 1328.0\n",
      "Topic 0 updated_retain_activations.norm= 1184.0\n",
      "Topic 0 frozen_retain_activations.norm= 1184.0\n",
      " 56%|███████████████████████                  | 169/300 [02:38<02:04,  1.05it/s]loss: 1536 | unlearn_loss: 1536 | retain_loss: 0.02966 | param_change: 1.609e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1808.0\n",
      "Topic 0 frozen_forget_activations.norm= 1808.0\n",
      "Topic 0 updated_retain_activations.norm= 1312.0\n",
      "Topic 0 frozen_retain_activations.norm= 1312.0\n",
      " 57%|███████████████████████▏                 | 170/300 [02:39<02:04,  1.04it/s]loss: 940 | unlearn_loss: 940 | retain_loss: 0.03052 | param_change: 1.371e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 940.0\n",
      "Topic 0 frozen_forget_activations.norm= 940.0\n",
      "Topic 0 updated_retain_activations.norm= 1128.0\n",
      "Topic 0 frozen_retain_activations.norm= 1128.0\n",
      " 57%|███████████████████████▎                 | 171/300 [02:40<01:56,  1.11it/s]loss: 1424 | unlearn_loss: 1424 | retain_loss: 0.03809 | param_change: 1.633e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1464.0\n",
      "Topic 0 frozen_forget_activations.norm= 1464.0\n",
      "Topic 0 updated_retain_activations.norm= 1264.0\n",
      "Topic 0 frozen_retain_activations.norm= 1264.0\n",
      " 57%|███████████████████████▌                 | 172/300 [02:41<01:54,  1.12it/s]loss: 952 | unlearn_loss: 952 | retain_loss: 0.0415 | param_change: 1.413e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 940.0\n",
      "Topic 0 frozen_forget_activations.norm= 940.0\n",
      "Topic 0 updated_retain_activations.norm= 1512.0\n",
      "Topic 0 frozen_retain_activations.norm= 1512.0\n",
      " 58%|███████████████████████▋                 | 173/300 [02:42<01:53,  1.12it/s]loss: 1304 | unlearn_loss: 1304 | retain_loss: 0.03491 | param_change: 2.038e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1328.0\n",
      "Topic 0 frozen_forget_activations.norm= 1328.0\n",
      "Topic 0 updated_retain_activations.norm= 904.0\n",
      "Topic 0 frozen_retain_activations.norm= 904.0\n",
      " 58%|███████████████████████▊                 | 174/300 [02:43<01:53,  1.11it/s]loss: 1368 | unlearn_loss: 1368 | retain_loss: 0.03149 | param_change: 1.448e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1424.0\n",
      "Topic 0 frozen_forget_activations.norm= 1424.0\n",
      "Topic 0 updated_retain_activations.norm= 1544.0\n",
      "Topic 0 frozen_retain_activations.norm= 1544.0\n",
      " 58%|███████████████████████▉                 | 175/300 [02:44<01:53,  1.10it/s]loss: 1496 | unlearn_loss: 1496 | retain_loss: 0.03394 | param_change: 1.252e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1704.0\n",
      "Topic 0 frozen_forget_activations.norm= 1704.0\n",
      "Topic 0 updated_retain_activations.norm= 1632.0\n",
      "Topic 0 frozen_retain_activations.norm= 1632.0\n",
      " 59%|████████████████████████                 | 176/300 [02:45<01:56,  1.07it/s]loss: 996 | unlearn_loss: 996 | retain_loss: 0.03418 | param_change: 1.681e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 956.0\n",
      "Topic 0 frozen_forget_activations.norm= 956.0\n",
      "Topic 0 updated_retain_activations.norm= 1288.0\n",
      "Topic 0 frozen_retain_activations.norm= 1288.0\n",
      " 59%|████████████████████████▏                | 177/300 [02:45<01:51,  1.10it/s]loss: 1344 | unlearn_loss: 1344 | retain_loss: 0.02185 | param_change: 1.466e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1432.0\n",
      "Topic 0 frozen_forget_activations.norm= 1432.0\n",
      "Topic 0 updated_retain_activations.norm= 1920.0\n",
      "Topic 0 frozen_retain_activations.norm= 1920.0\n",
      " 59%|████████████████████████▎                | 178/300 [02:47<02:06,  1.03s/it]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 384, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1392 | unlearn_loss: 1392 | retain_loss: 0.03442 | param_change: 1.425e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1544.0\n",
      "Topic 0 frozen_forget_activations.norm= 1544.0\n",
      "Topic 0 updated_retain_activations.norm= 996.0\n",
      "Topic 0 frozen_retain_activations.norm= 996.0\n",
      " 60%|████████████████████████▍                | 179/300 [02:48<02:03,  1.02s/it]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 182, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 724 | unlearn_loss: 724 | retain_loss: 0.02698 | param_change: 1.448e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 700.0\n",
      "Topic 0 frozen_forget_activations.norm= 700.0\n",
      "Topic 0 updated_retain_activations.norm= 1520.0\n",
      "Topic 0 frozen_retain_activations.norm= 1520.0\n",
      " 60%|████████████████████████▌                | 180/300 [02:49<01:52,  1.06it/s]loss: 1424 | unlearn_loss: 1424 | retain_loss: 0.03442 | param_change: 1.472e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1520.0\n",
      "Topic 0 frozen_forget_activations.norm= 1520.0\n",
      "Topic 0 updated_retain_activations.norm= 1232.0\n",
      "Topic 0 frozen_retain_activations.norm= 1232.0\n",
      " 60%|████████████████████████▋                | 181/300 [02:50<01:57,  1.02it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 298, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1448 | unlearn_loss: 1448 | retain_loss: 0.02637 | param_change: 1.705e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=0.99609375\n",
      "Topic 0 updated_forget_activations.norm= 1544.0\n",
      "Topic 0 frozen_forget_activations.norm= 1544.0\n",
      "Topic 0 updated_retain_activations.norm= 1840.0\n",
      "Topic 0 frozen_retain_activations.norm= 1840.0\n",
      " 61%|████████████████████████▊                | 182/300 [02:51<02:04,  1.05s/it]loss: 1384 | unlearn_loss: 1384 | retain_loss: 0.02087 | param_change: 1.657e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1448.0\n",
      "Topic 0 frozen_forget_activations.norm= 1448.0\n",
      "Topic 0 updated_retain_activations.norm= 2000.0\n",
      "Topic 0 frozen_retain_activations.norm= 2000.0\n",
      " 61%|█████████████████████████                | 183/300 [02:52<02:13,  1.14s/it]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 474, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1472 | unlearn_loss: 1472 | retain_loss: 0.03345 | param_change: 2.134e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1880.0\n",
      "Topic 0 frozen_forget_activations.norm= 1880.0\n",
      "Topic 0 updated_retain_activations.norm= 1296.0\n",
      "Topic 0 frozen_retain_activations.norm= 1296.0\n",
      " 61%|█████████████████████████▏               | 184/300 [02:54<02:19,  1.21s/it]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 380, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1480 | unlearn_loss: 1480 | retain_loss: 0.03955 | param_change: 1.788e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1688.0\n",
      "Topic 0 frozen_forget_activations.norm= 1688.0\n",
      "Topic 0 updated_retain_activations.norm= 1728.0\n",
      "Topic 0 frozen_retain_activations.norm= 1728.0\n",
      " 62%|█████████████████████████▎               | 185/300 [02:55<02:16,  1.19s/it]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 492, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1448 | unlearn_loss: 1448 | retain_loss: 0.03223 | param_change: 1.186e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1840.0\n",
      "Topic 0 frozen_forget_activations.norm= 1840.0\n",
      "Topic 0 updated_retain_activations.norm= 1120.0\n",
      "Topic 0 frozen_retain_activations.norm= 1120.0\n",
      " 62%|█████████████████████████▍               | 186/300 [02:56<02:17,  1.21s/it]loss: 1416 | unlearn_loss: 1416 | retain_loss: 0.02747 | param_change: 1.46e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1600.0\n",
      "Topic 0 frozen_forget_activations.norm= 1600.0\n",
      "Topic 0 updated_retain_activations.norm= 1680.0\n",
      "Topic 0 frozen_retain_activations.norm= 1680.0\n",
      " 62%|█████████████████████████▌               | 187/300 [02:57<02:09,  1.15s/it]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 277, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1416 | unlearn_loss: 1416 | retain_loss: 0.04102 | param_change: 4.458e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1504.0\n",
      "Topic 0 frozen_forget_activations.norm= 1504.0\n",
      "Topic 0 updated_retain_activations.norm= 852.0\n",
      "Topic 0 frozen_retain_activations.norm= 852.0\n",
      " 63%|█████████████████████████▋               | 188/300 [02:58<01:57,  1.05s/it]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 266, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1400 | unlearn_loss: 1400 | retain_loss: 0.03174 | param_change: 1.478e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1456.0\n",
      "Topic 0 frozen_forget_activations.norm= 1456.0\n",
      "Topic 0 updated_retain_activations.norm= 1208.0\n",
      "Topic 0 frozen_retain_activations.norm= 1208.0\n",
      " 63%|█████████████████████████▊               | 189/300 [02:59<01:49,  1.02it/s]loss: 1400 | unlearn_loss: 1400 | retain_loss: 0.02942 | param_change: 1.931e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1440.0\n",
      "Topic 0 frozen_forget_activations.norm= 1440.0\n",
      "Topic 0 updated_retain_activations.norm= 1840.0\n",
      "Topic 0 frozen_retain_activations.norm= 1840.0\n",
      " 63%|█████████████████████████▉               | 190/300 [02:59<01:43,  1.07it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 291, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1072 | unlearn_loss: 1072 | retain_loss: 0.04272 | param_change: 1.872e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1096.0\n",
      "Topic 0 frozen_forget_activations.norm= 1096.0\n",
      "Topic 0 updated_retain_activations.norm= 868.0\n",
      "Topic 0 frozen_retain_activations.norm= 868.0\n",
      " 64%|██████████████████████████               | 191/300 [03:00<01:39,  1.10it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 264, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1144 | unlearn_loss: 1144 | retain_loss: 0.03857 | param_change: 2.337e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1136.0\n",
      "Topic 0 frozen_forget_activations.norm= 1136.0\n",
      "Topic 0 updated_retain_activations.norm= 1464.0\n",
      "Topic 0 frozen_retain_activations.norm= 1464.0\n",
      " 64%|██████████████████████████▏              | 192/300 [03:01<01:38,  1.09it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 362, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1168 | unlearn_loss: 1168 | retain_loss: 0.04272 | param_change: 1.633e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1224.0\n",
      "Topic 0 frozen_forget_activations.norm= 1224.0\n",
      "Topic 0 updated_retain_activations.norm= 1504.0\n",
      "Topic 0 frozen_retain_activations.norm= 1504.0\n",
      " 64%|██████████████████████████▍              | 193/300 [03:02<01:41,  1.06it/s]loss: 1416 | unlearn_loss: 1416 | retain_loss: 0.03369 | param_change: 1.311e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1784.0\n",
      "Topic 0 frozen_forget_activations.norm= 1784.0\n",
      "Topic 0 updated_retain_activations.norm= 1544.0\n",
      "Topic 0 frozen_retain_activations.norm= 1544.0\n",
      " 65%|██████████████████████████▌              | 194/300 [03:04<02:00,  1.14s/it]loss: 1456 | unlearn_loss: 1456 | retain_loss: 0.04297 | param_change: 1.478e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1640.0\n",
      "Topic 0 frozen_forget_activations.norm= 1640.0\n",
      "Topic 0 updated_retain_activations.norm= 1384.0\n",
      "Topic 0 frozen_retain_activations.norm= 1384.0\n",
      " 65%|██████████████████████████▋              | 195/300 [03:05<01:56,  1.11s/it]loss: 1096 | unlearn_loss: 1096 | retain_loss: 0.02576 | param_change: 1.621e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1064.0\n",
      "Topic 0 frozen_forget_activations.norm= 1064.0\n",
      "Topic 0 updated_retain_activations.norm= 1904.0\n",
      "Topic 0 frozen_retain_activations.norm= 1904.0\n",
      " 65%|██████████████████████████▊              | 196/300 [03:06<02:02,  1.18s/it]loss: 1328 | unlearn_loss: 1328 | retain_loss: 0.03369 | param_change: 1.836e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1424.0\n",
      "Topic 0 frozen_forget_activations.norm= 1424.0\n",
      "Topic 0 updated_retain_activations.norm= 1208.0\n",
      "Topic 0 frozen_retain_activations.norm= 1208.0\n",
      " 66%|██████████████████████████▉              | 197/300 [03:07<01:53,  1.10s/it]loss: 916 | unlearn_loss: 916 | retain_loss: 0.0354 | param_change: 1.466e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 920.0\n",
      "Topic 0 frozen_forget_activations.norm= 920.0\n",
      "Topic 0 updated_retain_activations.norm= 1608.0\n",
      "Topic 0 frozen_retain_activations.norm= 1608.0\n",
      " 66%|███████████████████████████              | 198/300 [03:08<01:48,  1.06s/it]loss: 1216 | unlearn_loss: 1216 | retain_loss: 0.03589 | param_change: 1.884e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1272.0\n",
      "Topic 0 frozen_forget_activations.norm= 1272.0\n",
      "Topic 0 updated_retain_activations.norm= 1400.0\n",
      "Topic 0 frozen_retain_activations.norm= 1400.0\n",
      " 66%|███████████████████████████▏             | 199/300 [03:09<01:48,  1.07s/it]loss: 1424 | unlearn_loss: 1424 | retain_loss: 0.04126 | param_change: 1.383e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1520.0\n",
      "Topic 0 frozen_forget_activations.norm= 1520.0\n",
      "Topic 0 updated_retain_activations.norm= 1456.0\n",
      "Topic 0 frozen_retain_activations.norm= 1456.0\n",
      " 67%|███████████████████████████▎             | 200/300 [03:10<01:44,  1.05s/it]loss: 1184 | unlearn_loss: 1184 | retain_loss: 0.04663 | param_change: 1.752e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1160.0\n",
      "Topic 0 frozen_forget_activations.norm= 1160.0\n",
      "Topic 0 updated_retain_activations.norm= 1288.0\n",
      "Topic 0 frozen_retain_activations.norm= 1288.0\n",
      " 67%|███████████████████████████▍             | 201/300 [03:11<01:37,  1.02it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 506, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1416 | unlearn_loss: 1416 | retain_loss: 0.03979 | param_change: 1.693e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1784.0\n",
      "Topic 0 frozen_forget_activations.norm= 1784.0\n",
      "Topic 0 updated_retain_activations.norm= 800.0\n",
      "Topic 0 frozen_retain_activations.norm= 800.0\n",
      " 67%|███████████████████████████▌             | 202/300 [03:12<01:42,  1.05s/it]loss: 1456 | unlearn_loss: 1456 | retain_loss: 0.03467 | param_change: 1.526e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1496.0\n",
      "Topic 0 frozen_forget_activations.norm= 1496.0\n",
      "Topic 0 updated_retain_activations.norm= 1568.0\n",
      "Topic 0 frozen_retain_activations.norm= 1568.0\n",
      " 68%|███████████████████████████▋             | 203/300 [03:13<01:36,  1.00it/s]loss: 920 | unlearn_loss: 920 | retain_loss: 0.04004 | param_change: 1.472e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 892.0\n",
      "Topic 0 frozen_forget_activations.norm= 892.0\n",
      "Topic 0 updated_retain_activations.norm= 1664.0\n",
      "Topic 0 frozen_retain_activations.norm= 1664.0\n",
      " 68%|███████████████████████████▉             | 204/300 [03:14<01:31,  1.05it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 233, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1272 | unlearn_loss: 1272 | retain_loss: 0.04395 | param_change: 2.003e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1288.0\n",
      "Topic 0 frozen_forget_activations.norm= 1288.0\n",
      "Topic 0 updated_retain_activations.norm= 1232.0\n",
      "Topic 0 frozen_retain_activations.norm= 1232.0\n",
      " 68%|████████████████████████████             | 205/300 [03:15<01:28,  1.08it/s]loss: 1360 | unlearn_loss: 1360 | retain_loss: 0.04297 | param_change: 3.147e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1456.0\n",
      "Topic 0 frozen_forget_activations.norm= 1456.0\n",
      "Topic 0 updated_retain_activations.norm= 1208.0\n",
      "Topic 0 frozen_retain_activations.norm= 1208.0\n",
      " 69%|████████████████████████████▏            | 206/300 [03:16<01:25,  1.09it/s]loss: 1464 | unlearn_loss: 1464 | retain_loss: 0.03638 | param_change: 2.658e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1504.0\n",
      "Topic 0 frozen_forget_activations.norm= 1504.0\n",
      "Topic 0 updated_retain_activations.norm= 1480.0\n",
      "Topic 0 frozen_retain_activations.norm= 1480.0\n",
      " 69%|████████████████████████████▎            | 207/300 [03:16<01:22,  1.13it/s]loss: 1004 | unlearn_loss: 1004 | retain_loss: 0.03809 | param_change: 1.621e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 968.0\n",
      "Topic 0 frozen_forget_activations.norm= 968.0\n",
      "Topic 0 updated_retain_activations.norm= 1440.0\n",
      "Topic 0 frozen_retain_activations.norm= 1440.0\n",
      " 69%|████████████████████████████▍            | 208/300 [03:18<01:25,  1.08it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 356, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1344 | unlearn_loss: 1344 | retain_loss: 0.04541 | param_change: 1.395e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1496.0\n",
      "Topic 0 frozen_forget_activations.norm= 1496.0\n",
      "Topic 0 updated_retain_activations.norm= 1456.0\n",
      "Topic 0 frozen_retain_activations.norm= 1456.0\n",
      " 70%|████████████████████████████▌            | 209/300 [03:19<01:31,  1.01s/it]loss: 1040 | unlearn_loss: 1040 | retain_loss: 0.04468 | param_change: 1.472e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1032.0\n",
      "Topic 0 frozen_forget_activations.norm= 1032.0\n",
      "Topic 0 updated_retain_activations.norm= 1216.0\n",
      "Topic 0 frozen_retain_activations.norm= 1216.0\n",
      " 70%|████████████████████████████▋            | 210/300 [03:19<01:22,  1.09it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 310, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1176 | unlearn_loss: 1176 | retain_loss: 0.03662 | param_change: 1.574e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1192.0\n",
      "Topic 0 frozen_forget_activations.norm= 1192.0\n",
      "Topic 0 updated_retain_activations.norm= 1104.0\n",
      "Topic 0 frozen_retain_activations.norm= 1104.0\n",
      " 70%|████████████████████████████▊            | 211/300 [03:20<01:21,  1.09it/s]loss: 872 | unlearn_loss: 872 | retain_loss: 0.04028 | param_change: 1.562e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 864.0\n",
      "Topic 0 frozen_forget_activations.norm= 864.0\n",
      "Topic 0 updated_retain_activations.norm= 1232.0\n",
      "Topic 0 frozen_retain_activations.norm= 1232.0\n",
      " 71%|████████████████████████████▉            | 212/300 [03:21<01:16,  1.16it/s]loss: 1480 | unlearn_loss: 1480 | retain_loss: 0.03296 | param_change: 1.46e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1664.0\n",
      "Topic 0 frozen_forget_activations.norm= 1664.0\n",
      "Topic 0 updated_retain_activations.norm= 1704.0\n",
      "Topic 0 frozen_retain_activations.norm= 1704.0\n",
      " 71%|█████████████████████████████            | 213/300 [03:22<01:27,  1.01s/it]loss: 1488 | unlearn_loss: 1488 | retain_loss: 0.03882 | param_change: 1.425e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1952.0\n",
      "Topic 0 frozen_forget_activations.norm= 1952.0\n",
      "Topic 0 updated_retain_activations.norm= 1336.0\n",
      "Topic 0 frozen_retain_activations.norm= 1336.0\n",
      " 71%|█████████████████████████████▏           | 214/300 [03:24<01:36,  1.13s/it]loss: 1368 | unlearn_loss: 1368 | retain_loss: 0.0415 | param_change: 2.062e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1488.0\n",
      "Topic 0 frozen_forget_activations.norm= 1488.0\n",
      "Topic 0 updated_retain_activations.norm= 892.0\n",
      "Topic 0 frozen_retain_activations.norm= 892.0\n",
      " 72%|█████████████████████████████▍           | 215/300 [03:25<01:27,  1.03s/it]loss: 1488 | unlearn_loss: 1488 | retain_loss: 0.03955 | param_change: 1.371e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1672.0\n",
      "Topic 0 frozen_forget_activations.norm= 1672.0\n",
      "Topic 0 updated_retain_activations.norm= 1048.0\n",
      "Topic 0 frozen_retain_activations.norm= 1048.0\n",
      " 72%|█████████████████████████████▌           | 216/300 [03:26<01:25,  1.01s/it]loss: 1256 | unlearn_loss: 1256 | retain_loss: 0.0498 | param_change: 1.597e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1296.0\n",
      "Topic 0 frozen_forget_activations.norm= 1296.0\n",
      "Topic 0 updated_retain_activations.norm= 1312.0\n",
      "Topic 0 frozen_retain_activations.norm= 1312.0\n",
      " 72%|█████████████████████████████▋           | 217/300 [03:27<01:23,  1.01s/it]loss: 1000 | unlearn_loss: 1000 | retain_loss: 0.03564 | param_change: 1.454e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 992.0\n",
      "Topic 0 frozen_forget_activations.norm= 992.0\n",
      "Topic 0 updated_retain_activations.norm= 1200.0\n",
      "Topic 0 frozen_retain_activations.norm= 1200.0\n",
      " 73%|█████████████████████████████▊           | 218/300 [03:27<01:16,  1.07it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 343, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1312 | unlearn_loss: 1312 | retain_loss: 0.04395 | param_change: 1.74e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1360.0\n",
      "Topic 0 frozen_forget_activations.norm= 1360.0\n",
      "Topic 0 updated_retain_activations.norm= 1136.0\n",
      "Topic 0 frozen_retain_activations.norm= 1136.0\n",
      " 73%|█████████████████████████████▉           | 219/300 [03:28<01:19,  1.02it/s]loss: 1048 | unlearn_loss: 1048 | retain_loss: 0.03467 | param_change: 1.383e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1040.0\n",
      "Topic 0 frozen_forget_activations.norm= 1040.0\n",
      "Topic 0 updated_retain_activations.norm= 1640.0\n",
      "Topic 0 frozen_retain_activations.norm= 1640.0\n",
      " 73%|██████████████████████████████           | 220/300 [03:29<01:16,  1.04it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 213, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 848 | unlearn_loss: 848 | retain_loss: 0.04004 | param_change: 1.454e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 848.0\n",
      "Topic 0 frozen_forget_activations.norm= 848.0\n",
      "Topic 0 updated_retain_activations.norm= 1528.0\n",
      "Topic 0 frozen_retain_activations.norm= 1528.0\n",
      " 74%|██████████████████████████████▏          | 221/300 [03:30<01:13,  1.07it/s]loss: 1288 | unlearn_loss: 1288 | retain_loss: 0.03174 | param_change: 1.448e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1416.0\n",
      "Topic 0 frozen_forget_activations.norm= 1416.0\n",
      "Topic 0 updated_retain_activations.norm= 1976.0\n",
      "Topic 0 frozen_retain_activations.norm= 1976.0\n",
      " 74%|██████████████████████████████▎          | 222/300 [03:32<01:25,  1.10s/it]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 409, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1352 | unlearn_loss: 1352 | retain_loss: 0.04224 | param_change: 1.693e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1488.0\n",
      "Topic 0 frozen_forget_activations.norm= 1488.0\n",
      "Topic 0 updated_retain_activations.norm= 992.0\n",
      "Topic 0 frozen_retain_activations.norm= 992.0\n",
      " 74%|██████████████████████████████▍          | 223/300 [03:33<01:24,  1.10s/it]loss: 1352 | unlearn_loss: 1352 | retain_loss: 0.03589 | param_change: 1.562e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1392.0\n",
      "Topic 0 frozen_forget_activations.norm= 1392.0\n",
      "Topic 0 updated_retain_activations.norm= 1528.0\n",
      "Topic 0 frozen_retain_activations.norm= 1528.0\n",
      " 75%|██████████████████████████████▌          | 224/300 [03:34<01:20,  1.06s/it]loss: 1352 | unlearn_loss: 1352 | retain_loss: 0.03687 | param_change: 1.49e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1384.0\n",
      "Topic 0 frozen_forget_activations.norm= 1384.0\n",
      "Topic 0 updated_retain_activations.norm= 1656.0\n",
      "Topic 0 frozen_retain_activations.norm= 1656.0\n",
      " 75%|██████████████████████████████▊          | 225/300 [03:35<01:17,  1.04s/it]loss: 1200 | unlearn_loss: 1200 | retain_loss: 0.03638 | param_change: 5.317e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1208.0\n",
      "Topic 0 frozen_forget_activations.norm= 1208.0\n",
      "Topic 0 updated_retain_activations.norm= 1416.0\n",
      "Topic 0 frozen_retain_activations.norm= 1416.0\n",
      " 75%|██████████████████████████████▉          | 226/300 [03:36<01:11,  1.04it/s]loss: 1200 | unlearn_loss: 1200 | retain_loss: 0.04004 | param_change: 1.907e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1232.0\n",
      "Topic 0 frozen_forget_activations.norm= 1232.0\n",
      "Topic 0 updated_retain_activations.norm= 1560.0\n",
      "Topic 0 frozen_retain_activations.norm= 1560.0\n",
      " 76%|███████████████████████████████          | 227/300 [03:36<01:08,  1.06it/s]loss: 1216 | unlearn_loss: 1216 | retain_loss: 0.04492 | param_change: 1.776e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1224.0\n",
      "Topic 0 frozen_forget_activations.norm= 1224.0\n",
      "Topic 0 updated_retain_activations.norm= 1352.0\n",
      "Topic 0 frozen_retain_activations.norm= 1352.0\n",
      " 76%|███████████████████████████████▏         | 228/300 [03:37<01:05,  1.09it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 322, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1400 | unlearn_loss: 1400 | retain_loss: 0.03564 | param_change: 1.574e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1456.0\n",
      "Topic 0 frozen_forget_activations.norm= 1456.0\n",
      "Topic 0 updated_retain_activations.norm= 1360.0\n",
      "Topic 0 frozen_retain_activations.norm= 1360.0\n",
      " 76%|███████████████████████████████▎         | 229/300 [03:38<01:05,  1.08it/s]loss: 1480 | unlearn_loss: 1480 | retain_loss: 0.04858 | param_change: 2.038e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1656.0\n",
      "Topic 0 frozen_forget_activations.norm= 1656.0\n",
      "Topic 0 updated_retain_activations.norm= 1096.0\n",
      "Topic 0 frozen_retain_activations.norm= 1096.0\n",
      " 77%|███████████████████████████████▍         | 230/300 [03:39<01:06,  1.05it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 344, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1504 | unlearn_loss: 1504 | retain_loss: 0.04541 | param_change: 1.693e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1664.0\n",
      "Topic 0 frozen_forget_activations.norm= 1664.0\n",
      "Topic 0 updated_retain_activations.norm= 1088.0\n",
      "Topic 0 frozen_retain_activations.norm= 1088.0\n",
      " 77%|███████████████████████████████▌         | 231/300 [03:40<01:06,  1.04it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 372, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1376 | unlearn_loss: 1376 | retain_loss: 0.04126 | param_change: 1.478e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1520.0\n",
      "Topic 0 frozen_forget_activations.norm= 1520.0\n",
      "Topic 0 updated_retain_activations.norm= 1168.0\n",
      "Topic 0 frozen_retain_activations.norm= 1168.0\n",
      " 77%|███████████████████████████████▋         | 232/300 [03:41<01:06,  1.02it/s]loss: 1128 | unlearn_loss: 1128 | retain_loss: 0.03149 | param_change: 1.538e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1128.0\n",
      "Topic 0 frozen_forget_activations.norm= 1128.0\n",
      "Topic 0 updated_retain_activations.norm= 1800.0\n",
      "Topic 0 frozen_retain_activations.norm= 1800.0\n",
      " 78%|███████████████████████████████▊         | 233/300 [03:43<01:12,  1.08s/it]loss: 1368 | unlearn_loss: 1368 | retain_loss: 0.04492 | param_change: 1.496e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1472.0\n",
      "Topic 0 frozen_forget_activations.norm= 1472.0\n",
      "Topic 0 updated_retain_activations.norm= 1240.0\n",
      "Topic 0 frozen_retain_activations.norm= 1240.0\n",
      " 78%|███████████████████████████████▉         | 234/300 [03:43<01:08,  1.03s/it]loss: 1416 | unlearn_loss: 1416 | retain_loss: 0.03687 | param_change: 1.609e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1496.0\n",
      "Topic 0 frozen_forget_activations.norm= 1496.0\n",
      "Topic 0 updated_retain_activations.norm= 1496.0\n",
      "Topic 0 frozen_retain_activations.norm= 1496.0\n",
      " 78%|████████████████████████████████         | 235/300 [03:44<01:06,  1.02s/it]loss: 1032 | unlearn_loss: 1032 | retain_loss: 0.03369 | param_change: 1.323e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1056.0\n",
      "Topic 0 frozen_forget_activations.norm= 1056.0\n",
      "Topic 0 updated_retain_activations.norm= 1728.0\n",
      "Topic 0 frozen_retain_activations.norm= 1728.0\n",
      " 79%|████████████████████████████████▎        | 236/300 [03:45<01:02,  1.03it/s]loss: 1424 | unlearn_loss: 1424 | retain_loss: 0.04346 | param_change: 1.597e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1784.0\n",
      "Topic 0 frozen_forget_activations.norm= 1784.0\n",
      "Topic 0 updated_retain_activations.norm= 1240.0\n",
      "Topic 0 frozen_retain_activations.norm= 1240.0\n",
      " 79%|████████████████████████████████▍        | 237/300 [03:47<01:06,  1.05s/it]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 229, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1176 | unlearn_loss: 1176 | retain_loss: 0.03979 | param_change: 1.538e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1232.0\n",
      "Topic 0 frozen_forget_activations.norm= 1232.0\n",
      "Topic 0 updated_retain_activations.norm= 1384.0\n",
      "Topic 0 frozen_retain_activations.norm= 1384.0\n",
      " 79%|████████████████████████████████▌        | 238/300 [03:47<01:02,  1.01s/it]loss: 1168 | unlearn_loss: 1168 | retain_loss: 0.04858 | param_change: 1.508e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1192.0\n",
      "Topic 0 frozen_forget_activations.norm= 1192.0\n",
      "Topic 0 updated_retain_activations.norm= 820.0\n",
      "Topic 0 frozen_retain_activations.norm= 820.0\n",
      " 80%|████████████████████████████████▋        | 239/300 [03:48<00:58,  1.04it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 351, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1408 | unlearn_loss: 1408 | retain_loss: 0.03931 | param_change: 1.645e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1520.0\n",
      "Topic 0 frozen_forget_activations.norm= 1520.0\n",
      "Topic 0 updated_retain_activations.norm= 1240.0\n",
      "Topic 0 frozen_retain_activations.norm= 1240.0\n",
      " 80%|████████████████████████████████▊        | 240/300 [03:49<00:59,  1.00it/s]loss: 1456 | unlearn_loss: 1456 | retain_loss: 0.04468 | param_change: 1.872e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1600.0\n",
      "Topic 0 frozen_forget_activations.norm= 1600.0\n",
      "Topic 0 updated_retain_activations.norm= 1344.0\n",
      "Topic 0 frozen_retain_activations.norm= 1344.0\n",
      " 80%|████████████████████████████████▉        | 241/300 [03:50<00:56,  1.04it/s]loss: 1528 | unlearn_loss: 1528 | retain_loss: 0.05322 | param_change: 1.645e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1672.0\n",
      "Topic 0 frozen_forget_activations.norm= 1672.0\n",
      "Topic 0 updated_retain_activations.norm= 1344.0\n",
      "Topic 0 frozen_retain_activations.norm= 1344.0\n",
      " 81%|█████████████████████████████████        | 242/300 [03:51<00:55,  1.04it/s]loss: 1160 | unlearn_loss: 1160 | retain_loss: 0.04272 | param_change: 1.585e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1168.0\n",
      "Topic 0 frozen_forget_activations.norm= 1168.0\n",
      "Topic 0 updated_retain_activations.norm= 1128.0\n",
      "Topic 0 frozen_retain_activations.norm= 1128.0\n",
      " 81%|█████████████████████████████████▏       | 243/300 [03:52<00:51,  1.11it/s]loss: 1088 | unlearn_loss: 1088 | retain_loss: 0.04272 | param_change: 1.609e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1088.0\n",
      "Topic 0 frozen_forget_activations.norm= 1088.0\n",
      "Topic 0 updated_retain_activations.norm= 1344.0\n",
      "Topic 0 frozen_retain_activations.norm= 1344.0\n",
      " 81%|█████████████████████████████████▎       | 244/300 [03:53<00:48,  1.14it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 248, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1112 | unlearn_loss: 1112 | retain_loss: 0.04028 | param_change: 2.074e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1112.0\n",
      "Topic 0 frozen_forget_activations.norm= 1112.0\n",
      "Topic 0 updated_retain_activations.norm= 1544.0\n",
      "Topic 0 frozen_retain_activations.norm= 1544.0\n",
      " 82%|█████████████████████████████████▍       | 245/300 [03:54<00:46,  1.19it/s]loss: 1464 | unlearn_loss: 1464 | retain_loss: 0.03711 | param_change: 1.365e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1896.0\n",
      "Topic 0 frozen_forget_activations.norm= 1896.0\n",
      "Topic 0 updated_retain_activations.norm= 1328.0\n",
      "Topic 0 frozen_retain_activations.norm= 1328.0\n",
      " 82%|█████████████████████████████████▌       | 246/300 [03:55<00:52,  1.03it/s]loss: 1208 | unlearn_loss: 1208 | retain_loss: 0.04224 | param_change: 1.395e-05\n",
      "unlearn_cosine_sim=0.99609375\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1520.0\n",
      "Topic 0 frozen_forget_activations.norm= 1520.0\n",
      "Topic 0 updated_retain_activations.norm= 1896.0\n",
      "Topic 0 frozen_retain_activations.norm= 1896.0\n",
      " 82%|█████████████████████████████████▊       | 247/300 [03:56<00:58,  1.10s/it]loss: 1152 | unlearn_loss: 1152 | retain_loss: 0.04883 | param_change: 1.943e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1144.0\n",
      "Topic 0 frozen_forget_activations.norm= 1144.0\n",
      "Topic 0 updated_retain_activations.norm= 1152.0\n",
      "Topic 0 frozen_retain_activations.norm= 1152.0\n",
      " 83%|█████████████████████████████████▉       | 248/300 [03:57<00:52,  1.00s/it]loss: 1192 | unlearn_loss: 1192 | retain_loss: 0.04126 | param_change: 1.764e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1200.0\n",
      "Topic 0 frozen_forget_activations.norm= 1200.0\n",
      "Topic 0 updated_retain_activations.norm= 1408.0\n",
      "Topic 0 frozen_retain_activations.norm= 1408.0\n",
      " 83%|██████████████████████████████████       | 249/300 [03:58<00:49,  1.02it/s]loss: 852 | unlearn_loss: 852 | retain_loss: 0.04053 | param_change: 1.645e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 820.0\n",
      "Topic 0 frozen_forget_activations.norm= 820.0\n",
      "Topic 0 updated_retain_activations.norm= 1704.0\n",
      "Topic 0 frozen_retain_activations.norm= 1704.0\n",
      " 83%|██████████████████████████████████▏      | 250/300 [03:59<00:45,  1.09it/s]loss: 644 | unlearn_loss: 644 | retain_loss: 0.06738 | param_change: 2.205e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 600.0\n",
      "Topic 0 frozen_forget_activations.norm= 600.0\n",
      "Topic 0 updated_retain_activations.norm= 1040.0\n",
      "Topic 0 frozen_retain_activations.norm= 1040.0\n",
      " 84%|██████████████████████████████████▎      | 251/300 [04:00<00:43,  1.14it/s]loss: 1576 | unlearn_loss: 1576 | retain_loss: 0.05713 | param_change: 1.872e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1752.0\n",
      "Topic 0 frozen_forget_activations.norm= 1752.0\n",
      "Topic 0 updated_retain_activations.norm= 1024.0\n",
      "Topic 0 frozen_retain_activations.norm= 1024.0\n",
      " 84%|██████████████████████████████████▍      | 252/300 [04:00<00:42,  1.12it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 198, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 928 | unlearn_loss: 928 | retain_loss: 0.03198 | param_change: 1.442e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 924.0\n",
      "Topic 0 frozen_forget_activations.norm= 924.0\n",
      "Topic 0 updated_retain_activations.norm= 1888.0\n",
      "Topic 0 frozen_retain_activations.norm= 1888.0\n",
      " 84%|██████████████████████████████████▌      | 253/300 [04:02<00:46,  1.00it/s]loss: 1232 | unlearn_loss: 1232 | retain_loss: 0.04492 | param_change: 1.717e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1264.0\n",
      "Topic 0 frozen_forget_activations.norm= 1264.0\n",
      "Topic 0 updated_retain_activations.norm= 1112.0\n",
      "Topic 0 frozen_retain_activations.norm= 1112.0\n",
      " 85%|██████████████████████████████████▋      | 254/300 [04:03<00:45,  1.02it/s]loss: 1024 | unlearn_loss: 1024 | retain_loss: 0.04663 | param_change: 2.432e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1040.0\n",
      "Topic 0 frozen_forget_activations.norm= 1040.0\n",
      "Topic 0 updated_retain_activations.norm= 1024.0\n",
      "Topic 0 frozen_retain_activations.norm= 1024.0\n",
      " 85%|██████████████████████████████████▊      | 255/300 [04:03<00:41,  1.08it/s]loss: 1224 | unlearn_loss: 1224 | retain_loss: 0.05054 | param_change: 1.776e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1248.0\n",
      "Topic 0 frozen_forget_activations.norm= 1248.0\n",
      "Topic 0 updated_retain_activations.norm= 800.0\n",
      "Topic 0 frozen_retain_activations.norm= 800.0\n",
      " 85%|██████████████████████████████████▉      | 256/300 [04:04<00:40,  1.10it/s]loss: 1528 | unlearn_loss: 1528 | retain_loss: 0.0376 | param_change: 1.621e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1648.0\n",
      "Topic 0 frozen_forget_activations.norm= 1648.0\n",
      "Topic 0 updated_retain_activations.norm= 1520.0\n",
      "Topic 0 frozen_retain_activations.norm= 1520.0\n",
      " 86%|███████████████████████████████████      | 257/300 [04:05<00:37,  1.13it/s]loss: 1032 | unlearn_loss: 1032 | retain_loss: 0.04932 | param_change: 1.633e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1012.0\n",
      "Topic 0 frozen_forget_activations.norm= 1012.0\n",
      "Topic 0 updated_retain_activations.norm= 1264.0\n",
      "Topic 0 frozen_retain_activations.norm= 1264.0\n",
      " 86%|███████████████████████████████████▎     | 258/300 [04:06<00:37,  1.13it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 306, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1304 | unlearn_loss: 1304 | retain_loss: 0.05151 | param_change: 1.764e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1368.0\n",
      "Topic 0 frozen_forget_activations.norm= 1368.0\n",
      "Topic 0 updated_retain_activations.norm= 1496.0\n",
      "Topic 0 frozen_retain_activations.norm= 1496.0\n",
      " 86%|███████████████████████████████████▍     | 259/300 [04:07<00:38,  1.08it/s]loss: 892 | unlearn_loss: 892 | retain_loss: 0.05103 | param_change: 3.076e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 904.0\n",
      "Topic 0 frozen_forget_activations.norm= 904.0\n",
      "Topic 0 updated_retain_activations.norm= 1012.0\n",
      "Topic 0 frozen_retain_activations.norm= 1012.0\n",
      " 87%|███████████████████████████████████▌     | 260/300 [04:08<00:35,  1.13it/s]loss: 1440 | unlearn_loss: 1440 | retain_loss: 0.04688 | param_change: 1.657e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1512.0\n",
      "Topic 0 frozen_forget_activations.norm= 1512.0\n",
      "Topic 0 updated_retain_activations.norm= 1128.0\n",
      "Topic 0 frozen_retain_activations.norm= 1128.0\n",
      " 87%|███████████████████████████████████▋     | 261/300 [04:09<00:35,  1.09it/s]loss: 1328 | unlearn_loss: 1328 | retain_loss: 0.052 | param_change: 1.657e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1376.0\n",
      "Topic 0 frozen_forget_activations.norm= 1376.0\n",
      "Topic 0 updated_retain_activations.norm= 1168.0\n",
      "Topic 0 frozen_retain_activations.norm= 1168.0\n",
      " 87%|███████████████████████████████████▊     | 262/300 [04:10<00:33,  1.13it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 499, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1432 | unlearn_loss: 1432 | retain_loss: 0.03931 | param_change: 1.198e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1808.0\n",
      "Topic 0 frozen_forget_activations.norm= 1808.0\n",
      "Topic 0 updated_retain_activations.norm= 1184.0\n",
      "Topic 0 frozen_retain_activations.norm= 1184.0\n",
      " 88%|███████████████████████████████████▉     | 263/300 [04:11<00:38,  1.04s/it]loss: 1200 | unlearn_loss: 1200 | retain_loss: 0.05151 | param_change: 3.791e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1184.0\n",
      "Topic 0 frozen_forget_activations.norm= 1184.0\n",
      "Topic 0 updated_retain_activations.norm= 1112.0\n",
      "Topic 0 frozen_retain_activations.norm= 1112.0\n",
      " 88%|████████████████████████████████████     | 264/300 [04:12<00:35,  1.03it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 279, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1104 | unlearn_loss: 1104 | retain_loss: 0.05981 | param_change: 3.052e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1152.0\n",
      "Topic 0 frozen_forget_activations.norm= 1152.0\n",
      "Topic 0 updated_retain_activations.norm= 1264.0\n",
      "Topic 0 frozen_retain_activations.norm= 1264.0\n",
      " 88%|████████████████████████████████████▏    | 265/300 [04:13<00:33,  1.04it/s]loss: 1432 | unlearn_loss: 1432 | retain_loss: 0.04272 | param_change: 1.526e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1512.0\n",
      "Topic 0 frozen_forget_activations.norm= 1512.0\n",
      "Topic 0 updated_retain_activations.norm= 1712.0\n",
      "Topic 0 frozen_retain_activations.norm= 1712.0\n",
      " 89%|████████████████████████████████████▎    | 266/300 [04:14<00:33,  1.03it/s]loss: 828 | unlearn_loss: 828 | retain_loss: 0.03564 | param_change: 1.907e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 792.0\n",
      "Topic 0 frozen_forget_activations.norm= 792.0\n",
      "Topic 0 updated_retain_activations.norm= 1744.0\n",
      "Topic 0 frozen_retain_activations.norm= 1744.0\n",
      " 89%|████████████████████████████████████▍    | 267/300 [04:15<00:31,  1.03it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 470, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1496 | unlearn_loss: 1496 | retain_loss: 0.04346 | param_change: 1.729e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1920.0\n",
      "Topic 0 frozen_forget_activations.norm= 1920.0\n",
      "Topic 0 updated_retain_activations.norm= 1104.0\n",
      "Topic 0 frozen_retain_activations.norm= 1104.0\n",
      " 89%|████████████████████████████████████▋    | 268/300 [04:16<00:33,  1.04s/it]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 257, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1448 | unlearn_loss: 1448 | retain_loss: 0.04272 | param_change: 1.8e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1528.0\n",
      "Topic 0 frozen_forget_activations.norm= 1528.0\n",
      "Topic 0 updated_retain_activations.norm= 1344.0\n",
      "Topic 0 frozen_retain_activations.norm= 1344.0\n",
      " 90%|████████████████████████████████████▊    | 269/300 [04:17<00:32,  1.03s/it]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 197, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 752 | unlearn_loss: 752 | retain_loss: 0.0564 | param_change: 1.621e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 728.0\n",
      "Topic 0 frozen_forget_activations.norm= 728.0\n",
      "Topic 0 updated_retain_activations.norm= 1112.0\n",
      "Topic 0 frozen_retain_activations.norm= 1112.0\n",
      " 90%|████████████████████████████████████▉    | 270/300 [04:18<00:28,  1.04it/s]loss: 1280 | unlearn_loss: 1280 | retain_loss: 0.04346 | param_change: 1.729e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1288.0\n",
      "Topic 0 frozen_forget_activations.norm= 1288.0\n",
      "Topic 0 updated_retain_activations.norm= 1592.0\n",
      "Topic 0 frozen_retain_activations.norm= 1592.0\n",
      " 90%|█████████████████████████████████████    | 271/300 [04:19<00:28,  1.04it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 400, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1432 | unlearn_loss: 1432 | retain_loss: 0.03564 | param_change: 1.448e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1664.0\n",
      "Topic 0 frozen_forget_activations.norm= 1664.0\n",
      "Topic 0 updated_retain_activations.norm= 1640.0\n",
      "Topic 0 frozen_retain_activations.norm= 1640.0\n",
      " 91%|█████████████████████████████████████▏   | 272/300 [04:20<00:32,  1.15s/it]loss: 1128 | unlearn_loss: 1128 | retain_loss: 0.05444 | param_change: 1.907e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1144.0\n",
      "Topic 0 frozen_forget_activations.norm= 1144.0\n",
      "Topic 0 updated_retain_activations.norm= 1088.0\n",
      "Topic 0 frozen_retain_activations.norm= 1088.0\n",
      " 91%|█████████████████████████████████████▎   | 273/300 [04:21<00:28,  1.05s/it]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 440, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1520 | unlearn_loss: 1520 | retain_loss: 0.03857 | param_change: 1.395e-05\n",
      "unlearn_cosine_sim=0.99609375\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1928.0\n",
      "Topic 0 frozen_forget_activations.norm= 1928.0\n",
      "Topic 0 updated_retain_activations.norm= 1456.0\n",
      "Topic 0 frozen_retain_activations.norm= 1456.0\n",
      " 91%|█████████████████████████████████████▍   | 274/300 [04:22<00:28,  1.08s/it]loss: 896 | unlearn_loss: 896 | retain_loss: 0.0437 | param_change: 1.311e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 928.0\n",
      "Topic 0 frozen_forget_activations.norm= 928.0\n",
      "Topic 0 updated_retain_activations.norm= 1568.0\n",
      "Topic 0 frozen_retain_activations.norm= 1568.0\n",
      " 92%|█████████████████████████████████████▌   | 275/300 [04:23<00:25,  1.03s/it]loss: 792 | unlearn_loss: 792 | retain_loss: 0.04834 | param_change: 1.967e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 752.0\n",
      "Topic 0 frozen_forget_activations.norm= 752.0\n",
      "Topic 0 updated_retain_activations.norm= 1080.0\n",
      "Topic 0 frozen_retain_activations.norm= 1080.0\n",
      " 92%|█████████████████████████████████████▋   | 276/300 [04:24<00:23,  1.02it/s]loss: 1008 | unlearn_loss: 1008 | retain_loss: 0.03516 | param_change: 1.752e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1000.0\n",
      "Topic 0 frozen_forget_activations.norm= 1000.0\n",
      "Topic 0 updated_retain_activations.norm= 1816.0\n",
      "Topic 0 frozen_retain_activations.norm= 1816.0\n",
      " 92%|█████████████████████████████████████▊   | 277/300 [04:25<00:23,  1.02s/it]loss: 1440 | unlearn_loss: 1440 | retain_loss: 0.05005 | param_change: 1.681e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1504.0\n",
      "Topic 0 frozen_forget_activations.norm= 1504.0\n",
      "Topic 0 updated_retain_activations.norm= 1608.0\n",
      "Topic 0 frozen_retain_activations.norm= 1608.0\n",
      " 93%|█████████████████████████████████████▉   | 278/300 [04:26<00:23,  1.05s/it]loss: 1360 | unlearn_loss: 1360 | retain_loss: 0.0415 | param_change: 1.365e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1480.0\n",
      "Topic 0 frozen_forget_activations.norm= 1480.0\n",
      "Topic 0 updated_retain_activations.norm= 1384.0\n",
      "Topic 0 frozen_retain_activations.norm= 1384.0\n",
      " 93%|██████████████████████████████████████▏  | 279/300 [04:27<00:22,  1.08s/it]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 442, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1400 | unlearn_loss: 1400 | retain_loss: 0.04199 | param_change: 1.46e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1672.0\n",
      "Topic 0 frozen_forget_activations.norm= 1672.0\n",
      "Topic 0 updated_retain_activations.norm= 1688.0\n",
      "Topic 0 frozen_retain_activations.norm= 1688.0\n",
      " 93%|██████████████████████████████████████▎  | 280/300 [04:29<00:23,  1.16s/it]loss: 1328 | unlearn_loss: 1328 | retain_loss: 0.04834 | param_change: 1.884e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1352.0\n",
      "Topic 0 frozen_forget_activations.norm= 1352.0\n",
      "Topic 0 updated_retain_activations.norm= 1472.0\n",
      "Topic 0 frozen_retain_activations.norm= 1472.0\n",
      " 94%|██████████████████████████████████████▍  | 281/300 [04:30<00:20,  1.06s/it]loss: 1496 | unlearn_loss: 1496 | retain_loss: 0.04126 | param_change: 1.633e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1568.0\n",
      "Topic 0 frozen_forget_activations.norm= 1568.0\n",
      "Topic 0 updated_retain_activations.norm= 1584.0\n",
      "Topic 0 frozen_retain_activations.norm= 1584.0\n",
      " 94%|██████████████████████████████████████▌  | 282/300 [04:31<00:18,  1.01s/it]loss: 1240 | unlearn_loss: 1240 | retain_loss: 0.04175 | param_change: 1.812e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1288.0\n",
      "Topic 0 frozen_forget_activations.norm= 1288.0\n",
      "Topic 0 updated_retain_activations.norm= 1464.0\n",
      "Topic 0 frozen_retain_activations.norm= 1464.0\n",
      " 94%|██████████████████████████████████████▋  | 283/300 [04:31<00:16,  1.03it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 242, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1408 | unlearn_loss: 1408 | retain_loss: 0.05322 | param_change: 1.74e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1432.0\n",
      "Topic 0 frozen_forget_activations.norm= 1432.0\n",
      "Topic 0 updated_retain_activations.norm= 1472.0\n",
      "Topic 0 frozen_retain_activations.norm= 1472.0\n",
      " 95%|██████████████████████████████████████▊  | 284/300 [04:32<00:14,  1.08it/s]loss: 1056 | unlearn_loss: 1056 | retain_loss: 0.05029 | param_change: 1.919e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1064.0\n",
      "Topic 0 frozen_forget_activations.norm= 1064.0\n",
      "Topic 0 updated_retain_activations.norm= 1384.0\n",
      "Topic 0 frozen_retain_activations.norm= 1384.0\n",
      " 95%|██████████████████████████████████████▉  | 285/300 [04:33<00:13,  1.13it/s]loss: 1328 | unlearn_loss: 1328 | retain_loss: 0.05762 | param_change: 2.337e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1392.0\n",
      "Topic 0 frozen_forget_activations.norm= 1392.0\n",
      "Topic 0 updated_retain_activations.norm= 1248.0\n",
      "Topic 0 frozen_retain_activations.norm= 1248.0\n",
      " 95%|███████████████████████████████████████  | 286/300 [04:34<00:12,  1.15it/s]loss: 1304 | unlearn_loss: 1304 | retain_loss: 0.05444 | param_change: 1.8e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1336.0\n",
      "Topic 0 frozen_forget_activations.norm= 1336.0\n",
      "Topic 0 updated_retain_activations.norm= 840.0\n",
      "Topic 0 frozen_retain_activations.norm= 840.0\n",
      " 96%|███████████████████████████████████████▏ | 287/300 [04:35<00:10,  1.20it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 202, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1056 | unlearn_loss: 1056 | retain_loss: 0.03882 | param_change: 1.86e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1040.0\n",
      "Topic 0 frozen_forget_activations.norm= 1040.0\n",
      "Topic 0 updated_retain_activations.norm= 1648.0\n",
      "Topic 0 frozen_retain_activations.norm= 1648.0\n",
      " 96%|███████████████████████████████████████▎ | 288/300 [04:35<00:10,  1.20it/s]loss: 1120 | unlearn_loss: 1120 | retain_loss: 0.06128 | param_change: 1.931e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1120.0\n",
      "Topic 0 frozen_forget_activations.norm= 1120.0\n",
      "Topic 0 updated_retain_activations.norm= 1176.0\n",
      "Topic 0 frozen_retain_activations.norm= 1176.0\n",
      " 96%|███████████████████████████████████████▍ | 289/300 [04:36<00:09,  1.17it/s]loss: 1240 | unlearn_loss: 1240 | retain_loss: 0.0481 | param_change: 1.508e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1312.0\n",
      "Topic 0 frozen_forget_activations.norm= 1312.0\n",
      "Topic 0 updated_retain_activations.norm= 1192.0\n",
      "Topic 0 frozen_retain_activations.norm= 1192.0\n",
      " 97%|███████████████████████████████████████▋ | 290/300 [04:37<00:08,  1.19it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 467, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1472 | unlearn_loss: 1472 | retain_loss: 0.04761 | param_change: 1.407e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1856.0\n",
      "Topic 0 frozen_forget_activations.norm= 1856.0\n",
      "Topic 0 updated_retain_activations.norm= 1360.0\n",
      "Topic 0 frozen_retain_activations.norm= 1360.0\n",
      " 97%|███████████████████████████████████████▊ | 291/300 [04:38<00:08,  1.04it/s]loss: 1464 | unlearn_loss: 1464 | retain_loss: 0.03857 | param_change: 1.609e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1568.0\n",
      "Topic 0 frozen_forget_activations.norm= 1568.0\n",
      "Topic 0 updated_retain_activations.norm= 1712.0\n",
      "Topic 0 frozen_retain_activations.norm= 1712.0\n",
      " 97%|███████████████████████████████████████▉ | 292/300 [04:40<00:08,  1.01s/it]loss: 1328 | unlearn_loss: 1328 | retain_loss: 0.02795 | param_change: 1.574e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1448.0\n",
      "Topic 0 frozen_forget_activations.norm= 1448.0\n",
      "Topic 0 updated_retain_activations.norm= 1992.0\n",
      "Topic 0 frozen_retain_activations.norm= 1992.0\n",
      " 98%|████████████████████████████████████████ | 293/300 [04:41<00:07,  1.11s/it]loss: 984 | unlearn_loss: 984 | retain_loss: 0.0708 | param_change: 1.764e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 964.0\n",
      "Topic 0 frozen_forget_activations.norm= 964.0\n",
      "Topic 0 updated_retain_activations.norm= 1496.0\n",
      "Topic 0 frozen_retain_activations.norm= 1496.0\n",
      " 98%|████████████████████████████████████████▏| 294/300 [04:42<00:06,  1.04s/it]loss: 1368 | unlearn_loss: 1368 | retain_loss: 0.04541 | param_change: 1.669e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1408.0\n",
      "Topic 0 frozen_forget_activations.norm= 1408.0\n",
      "Topic 0 updated_retain_activations.norm= 796.0\n",
      "Topic 0 frozen_retain_activations.norm= 796.0\n",
      " 98%|████████████████████████████████████████▎| 295/300 [04:43<00:04,  1.03it/s]loss: 772 | unlearn_loss: 772 | retain_loss: 0.05933 | param_change: 1.74e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 744.0\n",
      "Topic 0 frozen_forget_activations.norm= 744.0\n",
      "Topic 0 updated_retain_activations.norm= 1376.0\n",
      "Topic 0 frozen_retain_activations.norm= 1376.0\n",
      " 99%|████████████████████████████████████████▍| 296/300 [04:43<00:03,  1.06it/s]loss: 1072 | unlearn_loss: 1072 | retain_loss: 0.06885 | param_change: 1.848e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1064.0\n",
      "Topic 0 frozen_forget_activations.norm= 1064.0\n",
      "Topic 0 updated_retain_activations.norm= 1480.0\n",
      "Topic 0 frozen_retain_activations.norm= 1480.0\n",
      " 99%|████████████████████████████████████████▌| 297/300 [04:44<00:02,  1.07it/s]/storage/ice1/6/7/jli928/model-unlearning/rmu/unlearn.py:68: UserWarning: Using a target size (torch.Size([1, 1, 4096])) that is different to the input size (torch.Size([8, 220, 4096])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
      "  unlearn_loss = torch.nn.functional.mse_loss(\n",
      "loss: 1160 | unlearn_loss: 1160 | retain_loss: 0.0564 | param_change: 1.764e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1152.0\n",
      "Topic 0 frozen_forget_activations.norm= 1152.0\n",
      "Topic 0 updated_retain_activations.norm= 1472.0\n",
      "Topic 0 frozen_retain_activations.norm= 1472.0\n",
      " 99%|████████████████████████████████████████▋| 298/300 [04:45<00:01,  1.09it/s]loss: 1376 | unlearn_loss: 1376 | retain_loss: 0.06396 | param_change: 1.764e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1688.0\n",
      "Topic 0 frozen_forget_activations.norm= 1688.0\n",
      "Topic 0 updated_retain_activations.norm= 1184.0\n",
      "Topic 0 frozen_retain_activations.norm= 1184.0\n",
      "100%|████████████████████████████████████████▊| 299/300 [04:47<00:01,  1.06s/it]loss: 1512 | unlearn_loss: 1512 | retain_loss: 0.05322 | param_change: 1.431e-05\n",
      "unlearn_cosine_sim=1.0\n",
      "retain_cosine_sim=1.0\n",
      "Topic 0 updated_forget_activations.norm= 1704.0\n",
      "Topic 0 frozen_forget_activations.norm= 1704.0\n",
      "Topic 0 updated_retain_activations.norm= 1392.0\n",
      "Topic 0 frozen_retain_activations.norm= 1392.0\n",
      "100%|█████████████████████████████████████████| 300/300 [04:48<00:00,  1.04it/s]\n",
      "Saved model to models/alpaca_rmu\n"
     ]
    }
   ],
   "source": [
    "# best\n",
    "import os\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0,1\"\n",
    "\n",
    "!python3 -m rmu.unlearn --model_name PKU-Alignment/alpaca-7b-reproduced --max_num_batches 300 --batch_size=8 --retain_corpora SafeRLHF-corpora/safe_train --forget_corpora SafeRLHF-corpora/Privacy_Violation_train --layer_ids 5,6,7 --layer_id 7 --steering_coeffs 6.5 --alpha 1200 --lr 1e-5 --seed 42 --output_dir models/alpaca_rmu --verbose"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "model-unlearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
